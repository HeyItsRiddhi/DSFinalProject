{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethnicity Classifier 2.0\n",
    "\n",
    "## Our story:\n",
    "\n",
    "We hypothesized that people's voting patterns may be varied along ethnic lines. In voter turnout literature, scientists have examined that the ethnic composition of neighborhoods (as available from U.S. Census data) and have found that certain ethnicities were more or less likely to vote (Chong & Kim, 2006).\n",
    "\n",
    "We want to test this conclusion with a different approach. Instead of looking at neighborhoods as a whole, we want to look at individuals. Neither U.S. Census Data nor Voter Records provide ethnicity data for individuals (although Voter Records provide a broad classification for \"race\", which is less specific than ethnicity). \n",
    "\n",
    "For each person, we will impute their ethnicity using a classifying algorithm that makes use of information within the Voter Records, and then we will compare the voting behaviors of different ethnic groups.\n",
    "\n",
    "We hope to take this additional imputed piece of data, and add it to our larger \"vote-turnout\" prediction model to (hopefully) increase its accuracy.\n",
    "\n",
    "\n",
    "## Our approach:\n",
    "\n",
    "** Model: ** We used a *Multinomial Logistic Regression* because:\n",
    "1. Our decision is categorical (non-binary)\n",
    "2. We can use the substrings within names as features, and let the classifier assign coefficients to them as they get correlated to ethnicities\n",
    "3. (Most Important) It allows us to output a score for each prediction, so we can tweak the threshold at which we go ahead and make a prediction. \n",
    "\n",
    "** Name-Ethnicity Datasets: ** We experiemented with two name-ethnicity datasets:\n",
    "1. A list of baby names found on FamilyEducation.com\n",
    "2. Names scraped from wikipedia that had ethnicity meta-data associated with them, open-sourced by (Ambekar, et al., 2009)\n",
    "\n",
    "** Test Set: ** Here we had to get creative. Because we didn't have actual ethnicities attached to the Voting Records, we needed to test externally. To make our test set, we:\n",
    "1. Took 10% of the total names in the wikipedia name-ethnicity dataset, and set them aside for testing\n",
    "2. Out of that dataset, we eliminated the names that appeared already in the training set. This makes our test set actually more stringent than the Voter Records set. This was necesary because: many of the names in the Voter Records were names that did not appear in our training set, so we didn't want to have an artificially high accuracy score in case names in the training set had a high propensity to re-appear within the training/test set.\n",
    "3. Used over-sampling to balance the test set.\n",
    "\n",
    "** Training Set: ** The remaining 90% names went to train our model. We over-sampled / under-sampled certain ethnicities to balance the training sets. We did not eliminate the repetition of names, because their frequencies are important because they correlate to real world frequencies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Ethnicity Classifier 1.0: Naive (Baseline) Model\n",
    "\n",
    "- **Trained on**: Lists of Baby Names.\n",
    "- **Classification decision**: Whichever ethnicity has the highest *proba* score. A decision is always made.\n",
    "\n",
    "### Ethnicity Classifier 1.1: New Training Data\n",
    "\n",
    "- **Trained on**: **Wikipedia-scraped name-ethnicity pairs.**\n",
    "- **Classification decision**: Whichever ethnicity has the highest *proba* score. A decision is always made.\n",
    "\n",
    "### Ethnicity Classifier 2.0: Race Used as an Input\n",
    "\n",
    "- **Trained on**: Wikipedia-scraped name-ethnicity pairs **& Race**.\n",
    "- **Classification decision**: Whichever ethnicity has the highest *proba* score. A decision is always made.\n",
    "\n",
    "### Ethnicity Classifier 2.1: Abstain option added\n",
    "\n",
    "- **Trained on**: Wikipedia-scraped name-ethnicity pairs & Race.\n",
    "- **Classification decision**: Only happens if *proba* exceeds a certain level, else we abstain from making a decision.\n",
    "\n",
    "\n",
    "\n",
    "Chong, D., & Kim, D. (2006). The experiences and effects of economic status among racial and ethnic minorities. American Political Science Review, 100(3), 335â€“351.\n",
    "\n",
    "Ambekar, A., Ward, C., Mohammed, J., Male, S., & Skiena, S. (2009, June). Name-ethnicity classification from open sources. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge Discovery and Data Mining (pp. 49-58). ACM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ethnicityguesser.NLTKMaxentEthnicityClassifier import NLTKMaxentEthnicityClassifier as mxec\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import & Clean Name Training/Testing Data\n",
    "\n",
    "Let's import and clean our name-ethnicity training sets.\n",
    "- List of Baby Names by Ethnicity\n",
    "- Name-ethnicity pairs scraped from Wikipedia\n",
    "\n",
    "Let's start with baby names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find names of files\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"ethnicityguesser/pickled_names\"):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "# list types of ethnicities\n",
    "ethnicities_baby = []\n",
    "for each in f:\n",
    "    ethnicities_baby.append(each.partition('.')[0])\n",
    "\n",
    "# pair type of ethnicity to its names in a dict\n",
    "eth_dict = {}\n",
    "for ethnicity in ethnicities_baby:\n",
    "    with open('ethnicityguesser/pickled_names/'+ethnicity+'.pkl', 'rb') as filename:\n",
    "        names = pickle.load(filename)\n",
    "    eth_dict[ethnicity] = names\n",
    "    \n",
    "ethnicities = ethnicities_baby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the ethnicities we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese',\n",
       " 'vietnamese',\n",
       " 'irish',\n",
       " 'danish',\n",
       " 'french',\n",
       " 'russian',\n",
       " 'japanese',\n",
       " 'german',\n",
       " 'czech',\n",
       " 'arabic',\n",
       " 'ukranian',\n",
       " 'swedish',\n",
       " 'spanish',\n",
       " 'african',\n",
       " 'swiss',\n",
       " 'korean',\n",
       " 'jewish',\n",
       " 'greek',\n",
       " 'italian',\n",
       " 'slavic',\n",
       " 'indian',\n",
       " 'muslim',\n",
       " 'portugese']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnicities_baby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets package these into a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a datafrome of names and true ethnicities\n",
    "\n",
    "super_list_names = []\n",
    "super_list_ethnicities = []\n",
    "\n",
    "for ethnicity in ethnicities:\n",
    "    name_list = eth_dict[ethnicity][0]\n",
    "    eth_list = []\n",
    "    for name in name_list:\n",
    "        eth_list.append(ethnicity)\n",
    "    super_list_names = super_list_names + name_list\n",
    "    super_list_ethnicities = super_list_ethnicities + eth_list\n",
    "    \n",
    "df_baby = pd.DataFrame(\n",
    "            {'Name': super_list_names,\n",
    "             'True Ethnicity': super_list_ethnicities\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5433</th>\n",
       "      <td>Thebeau</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16914</th>\n",
       "      <td>Anania</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18489</th>\n",
       "      <td>Gokhale</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>Jonassen</td>\n",
       "      <td>danish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14420</th>\n",
       "      <td>Ditzah</td>\n",
       "      <td>jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>Masson</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16491</th>\n",
       "      <td>Tabachnik</td>\n",
       "      <td>jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20145</th>\n",
       "      <td>Santiago</td>\n",
       "      <td>portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18301</th>\n",
       "      <td>Zachow</td>\n",
       "      <td>slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17896</th>\n",
       "      <td>Righi</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name True Ethnicity\n",
       "5433     Thebeau         french\n",
       "16914     Anania          greek\n",
       "18489    Gokhale         indian\n",
       "1227    Jonassen         danish\n",
       "14420     Ditzah         jewish\n",
       "4358      Masson         french\n",
       "16491  Tabachnik         jewish\n",
       "20145   Santiago      portugese\n",
       "18301     Zachow         slavic\n",
       "17896      Righi        italian"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baby.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import and clean the wikipedia name data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_middle</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98023</th>\n",
       "      <td>paul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lyn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterEuropean,British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39842</th>\n",
       "      <td>okumu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sibi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterAfrican,Africans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58182</th>\n",
       "      <td>iv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>honorÃ©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterEuropean,WestEuropean,French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131755</th>\n",
       "      <td>rao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rao</td>\n",
       "      <td>gopal</td>\n",
       "      <td>Asian,IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96192</th>\n",
       "      <td>furphy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ken</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterEuropean,British</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_last name_suffix name_first name_middle  \\\n",
       "98023       paul         NaN        lyn         NaN   \n",
       "39842      okumu         NaN       sibi         NaN   \n",
       "58182         iv         NaN     honorÃ©         NaN   \n",
       "131755       rao         NaN        rao       gopal   \n",
       "96192     furphy         NaN        ken         NaN   \n",
       "\n",
       "                                       race  \n",
       "98023               GreaterEuropean,British  \n",
       "39842               GreaterAfrican,Africans  \n",
       "58182   GreaterEuropean,WestEuropean,French  \n",
       "131755             Asian,IndianSubContinent  \n",
       "96192               GreaterEuropean,British  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_raw = pd.read_csv('wikipedia_data_scraped/wiki_name_race.csv')\n",
    "df_wiki_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yep. This is going to need some cleaning. Let's do this by:\n",
    "- Creating a new row for each name that is present (first, middle, last). We'll assume for not that the distinction is not important.\n",
    "- For ethnicity, let's only use the most specific ethnicity available to us (e.g. Italian instead of WestEuropean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_wiki_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_list_names = []\n",
    "super_list_ethnicities = []\n",
    "\n",
    "# clean names, simplify \"ethnicity\" field to just most specific one\n",
    "for row in range(len(df_wiki_raw)):\n",
    "    # filter valid first names\n",
    "    if type(df_wiki_raw.iloc[row].name_first) == str and len(df_wiki_raw.iloc[row].name_first) > 2:\n",
    "        super_list_names.append(df_wiki_raw.iloc[row].name_first)\n",
    "        super_list_ethnicities.append(df_wiki_raw.iloc[row].race.split(',')[-1])\n",
    "    # filter valid middle names\n",
    "    if type(df_wiki_raw.iloc[row].name_middle) == str and len(df_wiki_raw.iloc[row].name_middle) > 2:\n",
    "        super_list_names.append(df_wiki_raw.iloc[row].name_middle)\n",
    "        super_list_ethnicities.append(df_wiki_raw.iloc[row].race.split(',')[-1])\n",
    "    # filter valid last names\n",
    "    if type(df_wiki_raw.iloc[row].name_last) == str and len(df_wiki_raw.iloc[row].name_last) > 2:\n",
    "        super_list_names.append(df_wiki_raw.iloc[row].name_last)\n",
    "        super_list_ethnicities.append(df_wiki_raw.iloc[row].race.split(',')[-1])\n",
    "\n",
    "# throw it into a dataframe\n",
    "df_wiki = pd.DataFrame(\n",
    "            {'Name': super_list_names,\n",
    "             'True Ethnicity': super_list_ethnicities\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256259</th>\n",
       "      <td>abidi</td>\n",
       "      <td>IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206431</th>\n",
       "      <td>elmsley</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>muhammad</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252247</th>\n",
       "      <td>arun</td>\n",
       "      <td>IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195046</th>\n",
       "      <td>hunter</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104580</th>\n",
       "      <td>kuramoto</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244159</th>\n",
       "      <td>fan</td>\n",
       "      <td>EastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72702</th>\n",
       "      <td>weinberger</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154685</th>\n",
       "      <td>andrew</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169829</th>\n",
       "      <td>harley</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279857</th>\n",
       "      <td>sinimberghi</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21087</th>\n",
       "      <td>bin laden tape</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19056</th>\n",
       "      <td>nil</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241353</th>\n",
       "      <td>michitsura</td>\n",
       "      <td>EastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256468</th>\n",
       "      <td>d'cruz</td>\n",
       "      <td>IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275894</th>\n",
       "      <td>duchy of savoy</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215670</th>\n",
       "      <td>fanshawe</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215872</th>\n",
       "      <td>simon</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103052</th>\n",
       "      <td>hiratsuka</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60994</th>\n",
       "      <td>azaziah</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name      True Ethnicity\n",
       "256259           abidi  IndianSubContinent\n",
       "206431         elmsley             British\n",
       "18151         muhammad              Muslim\n",
       "252247            arun  IndianSubContinent\n",
       "195046          hunter             British\n",
       "104580        kuramoto            Japanese\n",
       "244159             fan           EastAsian\n",
       "72702       weinberger              Jewish\n",
       "154685          andrew             British\n",
       "169829          harley             British\n",
       "279857     sinimberghi             Italian\n",
       "21087   bin laden tape              Muslim\n",
       "19056              nil              Muslim\n",
       "241353      michitsura           EastAsian\n",
       "256468          d'cruz  IndianSubContinent\n",
       "275894  duchy of savoy             Italian\n",
       "215670        fanshawe             British\n",
       "215872           simon             British\n",
       "103052       hiratsuka            Japanese\n",
       "60994          azaziah              Jewish"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>295895</td>\n",
       "      <td>295895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>97507</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>john</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2910</td>\n",
       "      <td>88353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name True Ethnicity\n",
       "count   295895         295895\n",
       "unique   97507             13\n",
       "top       john        British\n",
       "freq      2910          88353"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine. Now let's standardize the ethnicity categories output in the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby: ['chinese', 'vietnamese', 'irish', 'danish', 'french', 'russian', 'japanese', 'german', 'czech', 'arabic', 'ukranian', 'swedish', 'spanish', 'african', 'swiss', 'korean', 'jewish', 'greek', 'italian', 'slavic', 'indian', 'muslim', 'portugese']\n"
     ]
    }
   ],
   "source": [
    "print \"Baby:\", ethnicities_baby\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby Names: ['chinese', 'vietnamese', 'irish', 'danish', 'french', 'russian', 'japanese', 'german', 'czech', 'arabic', 'ukranian', 'swedish', 'spanish', 'african', 'swiss', 'korean', 'jewish', 'greek', 'italian', 'slavic', 'indian', 'muslim', 'portugese']\n",
      "Wiki: ['Germanic' 'Muslim' 'Nordic' 'Hispanic' 'Jewish' 'Africans' 'Japanese'\n",
      " 'French' 'EastEuropean' 'British' 'EastAsian' 'IndianSubContinent'\n",
      " 'Italian']\n"
     ]
    }
   ],
   "source": [
    "ethnicities_wiki = df_wiki[\"True Ethnicity\"].unique()\n",
    "print \"Baby Names:\", ethnicities_baby\n",
    "print \"Wiki:\", ethnicities_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's standardize the ethnicities between our two datasets (the baby names dataset has ten more ethnicity categories than the Wikipedia dataset, 23 and 13 respectively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## standardize & consolidated ethnicities\n",
    "# format = c_eth[\"consolidated\"] = [[' eth from baby names'],['eth from wiki']]\n",
    "\n",
    "c_eth = {}\n",
    "\n",
    "c_eth[\"East European\"] = ['russian','ukranian','czech','slavic', 'greek', # baby names\n",
    "                          'EastEuropean'] # wiki names\n",
    "\n",
    "c_eth['West European'] = ['italian','irish','danish','french', 'swedish','german','swiss',\n",
    "                          'Nordic','British', 'Germanic', 'French', 'Italian'] \n",
    "\n",
    "c_eth['Muslim'] = ['muslim', 'arabic',\n",
    "                   'Muslim']\n",
    "\n",
    "c_eth['East Asian'] = ['chinese','japanese','vietnamese','korean',\n",
    "                       'EastAsian', 'Japanese']\n",
    "\n",
    "c_eth['Hispanic'] = ['spanish','portugese',\n",
    "                     'Hispanic']\n",
    "\n",
    "c_eth['Jewish'] = ['jewish','Jewish']\n",
    "\n",
    "c_eth['Indian'] = ['indian','IndianSubContinent']\n",
    "\n",
    "c_eth['Continental African'] = ['african','Africans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20245 20245 20245\n",
      "295895 295895 295895\n"
     ]
    }
   ],
   "source": [
    "## transform datasets\n",
    "def standardizeEth(df):\n",
    "    names = list(df['Name'])\n",
    "    org_eth = list(df['True Ethnicity'])    \n",
    "    standard_eth = []\n",
    "    for ethnicity in org_eth:\n",
    "        # search ethnicity dict\n",
    "        for c in c_eth:\n",
    "            # if found\n",
    "            if ethnicity in c_eth[c]:\n",
    "                # then add to master list\n",
    "                standard_eth.append(c)\n",
    "    print len(names), len(standard_eth), len(org_eth)\n",
    "    df_new = pd.DataFrame(\n",
    "            {'Name': names,\n",
    "             'True Ethnicity': org_eth,\n",
    "             'Standardized Ethnicity': standard_eth\n",
    "            })\n",
    "    return df_new\n",
    "\n",
    "df_baby_standard = standardizeEth(df_baby)\n",
    "\n",
    "df_wiki_standard = standardizeEth(df_wiki)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Standardized Ethnicity</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177607</th>\n",
       "      <td>davies</td>\n",
       "      <td>West European</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63333</th>\n",
       "      <td>rowe</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>roesler</td>\n",
       "      <td>West European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140517</th>\n",
       "      <td>patrushev</td>\n",
       "      <td>East European</td>\n",
       "      <td>EastEuropean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13045</th>\n",
       "      <td>valero</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name Standardized Ethnicity True Ethnicity\n",
       "177607     davies          West European        British\n",
       "63333        rowe                 Jewish         Jewish\n",
       "2108      roesler          West European       Germanic\n",
       "140517  patrushev          East European   EastEuropean\n",
       "13045      valero                 Muslim         Muslim"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_standard.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, nice data that we can use. Let's do one last step and balance the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "British               88353\n",
       "French                27566\n",
       "Italian               26711\n",
       "Hispanic              24469\n",
       "Jewish                22406\n",
       "EastEuropean          18311\n",
       "IndianSubContinent    17988\n",
       "Japanese              15906\n",
       "Muslim                14340\n",
       "EastAsian             11459\n",
       "Nordic                10927\n",
       "Germanic               8999\n",
       "Africans               8460\n",
       "Name: True Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Balance Wiki\n",
    "\n",
    "df_wiki_standard['True Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spanish', 'portugese', 'Hispanic']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_eth[c_eth.keys()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_wiki_balanced = pd.DataFrame(columns=['Name', 'Standardized Ethnicity','True Ethnicity'])\n",
    "\n",
    "##  balancing #1 - balance by True Ethnicity\n",
    "for eth in ethnicities_wiki:\n",
    "    # sample maximum amount from each(8460 - limit because of Africans)\n",
    "    sample_df = df_wiki_standard[df_wiki_standard['True Ethnicity']==eth].sample(8460)\n",
    "    df_wiki_balanced = pd.concat([df_wiki_balanced,sample_df])\n",
    "    \n",
    "## balancing #2 - balance by Standardized Eth, with equal numbers of True Eth in each group\n",
    "df_wiki_b = pd.DataFrame(columns=['Name', 'Standardized Ethnicity','True Ethnicity'])\n",
    "\n",
    "for eth in c_eth.keys():\n",
    "    sample_df = df_wiki_balanced[df_wiki_balanced['Standardized Ethnicity']==eth].sample(8460)\n",
    "    df_wiki_b = pd.concat([df_wiki_b,sample_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the proportions make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "West European          8460\n",
       "East Asian             8460\n",
       "Muslim                 8460\n",
       "Indian                 8460\n",
       "East European          8460\n",
       "Continental African    8460\n",
       "Jewish                 8460\n",
       "Hispanic               8460\n",
       "Name: Standardized Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_b['Standardized Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Muslim                8460\n",
       "Africans              8460\n",
       "Jewish                8460\n",
       "Hispanic              8460\n",
       "IndianSubContinent    8460\n",
       "EastEuropean          8460\n",
       "EastAsian             4258\n",
       "Japanese              4202\n",
       "British               1735\n",
       "Germanic              1713\n",
       "Nordic                1702\n",
       "French                1659\n",
       "Italian               1651\n",
       "Name: True Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_b['True Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, lets finish this up by splitting into test/training (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample (n) 67680\n",
      "Test Sample (test n) 13616\n",
      "Train Sample (train n) 54064\n"
     ]
    }
   ],
   "source": [
    "## Split into Training and Test\n",
    "msk = np.random.rand(len(df_wiki_b)) < 0.8\n",
    "train_df_w = df_wiki_b[msk]\n",
    "test_df_w = df_wiki_b[~msk]\n",
    "\n",
    "print \"Total Sample (n)\", len (df_wiki_b)\n",
    "print \"Test Sample (test n)\", len(test_df_w)\n",
    "print \"Train Sample (train n)\", len(train_df_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat for baby names df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "french        4143\n",
       "jewish        3076\n",
       "spanish       2366\n",
       "czech         1406\n",
       "swedish       1158\n",
       "portugese      834\n",
       "swiss          774\n",
       "german         723\n",
       "italian        711\n",
       "danish         656\n",
       "indian         580\n",
       "japanese       566\n",
       "muslim         525\n",
       "greek          429\n",
       "chinese        426\n",
       "ukranian       409\n",
       "irish          318\n",
       "african        300\n",
       "slavic         258\n",
       "russian        181\n",
       "korean         169\n",
       "vietnamese     129\n",
       "arabic         108\n",
       "Name: True Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baby_standard['True Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hispanic\n",
      "Jewish\n",
      "East Asian\n",
      "Muslim\n",
      "West European\n",
      "East European\n",
      "Indian\n",
      "Continental African\n"
     ]
    }
   ],
   "source": [
    "for each in c_eth:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Package DF into training token\n",
    "\n",
    "def makeTokens(ethnicities, train_df):\n",
    "    train_tokens = []\n",
    "    for ethnicity in ethnicities:\n",
    "        new_tokens = (list(train_df[train_df['Standardized Ethnicity'] == ethnicity]['Name']), ethnicity)\n",
    "        train_tokens.append(new_tokens)\n",
    "    return train_tokens\n",
    "\n",
    "wiki_tokens = makeTokens(c_eth, train_df_w)\n",
    "\n",
    "\n",
    "# (Tokens must be a list of ([list of names], 'ethnicity') pairs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Classifier (beware, this takes time)\n",
    "\n",
    "##classifier = mxec(wiki_tokens)\n",
    "##classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.38629        0.250\n",
      "             2          -0.89420        0.786\n",
      "             3          -0.70446        0.805\n",
      "             4          -0.60567        0.821\n",
      "             5          -0.54320        0.835\n",
      "             6          -0.49901        0.846\n",
      "             7          -0.46546        0.855\n",
      "             8          -0.43875        0.861\n",
      "             9          -0.41676        0.867\n",
      "            10          -0.39821        0.871\n",
      "            11          -0.38226        0.875\n",
      "            12          -0.36835        0.879\n",
      "            13          -0.35606        0.881\n",
      "            14          -0.34511        0.884\n",
      "            15          -0.33526        0.886\n",
      "            16          -0.32636        0.887\n",
      "            17          -0.31825        0.889\n",
      "            18          -0.31084        0.890\n",
      "            19          -0.30403        0.891\n",
      "            20          -0.29775        0.892\n",
      "            21          -0.29194        0.893\n",
      "            22          -0.28654        0.894\n",
      "            23          -0.28151        0.895\n",
      "            24          -0.27682        0.895\n",
      "            25          -0.27242        0.896\n",
      "            26          -0.26830        0.896\n",
      "            27          -0.26443        0.897\n",
      "            28          -0.26077        0.897\n",
      "            29          -0.25733        0.898\n",
      "            30          -0.25407        0.898\n",
      "            31          -0.25099        0.898\n",
      "            32          -0.24807        0.898\n",
      "            33          -0.24529        0.899\n",
      "            34          -0.24265        0.899\n",
      "            35          -0.24014        0.899\n",
      "            36          -0.23775        0.899\n",
      "            37          -0.23546        0.899\n",
      "            38          -0.23328        0.899\n",
      "            39          -0.23119        0.900\n",
      "            40          -0.22919        0.900\n",
      "            41          -0.22728        0.900\n",
      "            42          -0.22544        0.900\n",
      "            43          -0.22368        0.900\n",
      "            44          -0.22199        0.900\n",
      "            45          -0.22036        0.900\n",
      "            46          -0.21880        0.900\n",
      "            47          -0.21729        0.900\n",
      "            48          -0.21584        0.900\n",
      "            49          -0.21444        0.900\n",
      "            50          -0.21309        0.900\n",
      "            51          -0.21179        0.900\n",
      "            52          -0.21053        0.900\n",
      "            53          -0.20931        0.900\n",
      "            54          -0.20814        0.900\n",
      "            55          -0.20700        0.900\n",
      "            56          -0.20590        0.900\n",
      "            57          -0.20483        0.900\n",
      "            58          -0.20379        0.900\n",
      "            59          -0.20279        0.900\n",
      "            60          -0.20182        0.900\n",
      "            61          -0.20088        0.900\n",
      "            62          -0.19996        0.900\n",
      "            63          -0.19907        0.900\n",
      "            64          -0.19820        0.900\n",
      "            65          -0.19736        0.900\n",
      "            66          -0.19655        0.900\n",
      "            67          -0.19575        0.900\n",
      "            68          -0.19498        0.900\n",
      "            69          -0.19422        0.900\n",
      "            70          -0.19349        0.900\n",
      "            71          -0.19277        0.900\n",
      "            72          -0.19208        0.900\n",
      "            73          -0.19140        0.900\n",
      "            74          -0.19073        0.900\n",
      "            75          -0.19009        0.900\n",
      "            76          -0.18946        0.900\n",
      "            77          -0.18884        0.900\n",
      "            78          -0.18824        0.900\n",
      "            79          -0.18766        0.900\n",
      "            80          -0.18708        0.900\n",
      "            81          -0.18652        0.900\n",
      "            82          -0.18597        0.900\n",
      "            83          -0.18544        0.900\n",
      "            84          -0.18492        0.900\n",
      "            85          -0.18440        0.900\n",
      "            86          -0.18390        0.900\n",
      "            87          -0.18341        0.900\n",
      "            88          -0.18293        0.900\n",
      "            89          -0.18246        0.900\n",
      "            90          -0.18200        0.900\n",
      "            91          -0.18155        0.900\n",
      "            92          -0.18111        0.900\n",
      "            93          -0.18068        0.900\n",
      "            94          -0.18026        0.900\n",
      "            95          -0.17984        0.900\n",
      "            96          -0.17943        0.900\n",
      "            97          -0.17903        0.900\n",
      "            98          -0.17864        0.900\n",
      "            99          -0.17826        0.900\n",
      "         Final          -0.17788        0.900\n"
     ]
    }
   ],
   "source": [
    "## White\n",
    "white_tokens = makeTokens(['East European', 'West European', 'Jewish', 'Muslim'], train_df_w)\n",
    "white_classifier = mxec(white_tokens)\n",
    "white_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.333\n",
      "             2          -0.71315        0.818\n",
      "             3          -0.56806        0.837\n",
      "             4          -0.49074        0.852\n",
      "             5          -0.44072        0.865\n",
      "             6          -0.40470        0.875\n",
      "             7          -0.37699        0.883\n",
      "             8          -0.35472        0.891\n",
      "             9          -0.33625        0.896\n",
      "            10          -0.32060        0.901\n",
      "            11          -0.30709        0.905\n",
      "            12          -0.29527        0.909\n",
      "            13          -0.28482        0.911\n",
      "            14          -0.27549        0.913\n",
      "            15          -0.26710        0.915\n",
      "            16          -0.25950        0.917\n",
      "            17          -0.25259        0.918\n",
      "            18          -0.24626        0.919\n",
      "            19          -0.24044        0.920\n",
      "            20          -0.23508        0.921\n",
      "            21          -0.23011        0.922\n",
      "            22          -0.22550        0.922\n",
      "            23          -0.22121        0.923\n",
      "            24          -0.21720        0.923\n",
      "            25          -0.21345        0.924\n",
      "            26          -0.20993        0.924\n",
      "            27          -0.20662        0.924\n",
      "            28          -0.20350        0.925\n",
      "            29          -0.20056        0.925\n",
      "            30          -0.19778        0.925\n",
      "            31          -0.19515        0.925\n",
      "            32          -0.19266        0.925\n",
      "            33          -0.19029        0.926\n",
      "            34          -0.18804        0.926\n",
      "            35          -0.18589        0.926\n",
      "            36          -0.18385        0.926\n",
      "            37          -0.18191        0.926\n",
      "            38          -0.18004        0.926\n",
      "            39          -0.17827        0.926\n",
      "            40          -0.17656        0.926\n",
      "            41          -0.17493        0.926\n",
      "            42          -0.17337        0.926\n",
      "            43          -0.17187        0.926\n",
      "            44          -0.17043        0.926\n",
      "            45          -0.16904        0.926\n",
      "            46          -0.16771        0.926\n",
      "            47          -0.16643        0.926\n",
      "            48          -0.16519        0.927\n",
      "            49          -0.16400        0.927\n",
      "            50          -0.16285        0.927\n",
      "            51          -0.16174        0.927\n",
      "            52          -0.16067        0.927\n",
      "            53          -0.15964        0.927\n",
      "            54          -0.15863        0.927\n",
      "            55          -0.15767        0.927\n",
      "            56          -0.15673        0.927\n",
      "            57          -0.15582        0.927\n",
      "            58          -0.15494        0.927\n",
      "            59          -0.15409        0.927\n",
      "            60          -0.15326        0.927\n",
      "            61          -0.15246        0.927\n",
      "            62          -0.15168        0.927\n",
      "            63          -0.15092        0.927\n",
      "            64          -0.15019        0.927\n",
      "            65          -0.14947        0.927\n",
      "            66          -0.14878        0.927\n",
      "            67          -0.14810        0.927\n",
      "            68          -0.14744        0.927\n",
      "            69          -0.14680        0.927\n",
      "            70          -0.14618        0.927\n",
      "            71          -0.14557        0.927\n",
      "            72          -0.14498        0.927\n",
      "            73          -0.14440        0.927\n",
      "            74          -0.14384        0.927\n",
      "            75          -0.14329        0.927\n",
      "            76          -0.14275        0.927\n",
      "            77          -0.14223        0.927\n",
      "            78          -0.14172        0.927\n",
      "            79          -0.14122        0.927\n",
      "            80          -0.14074        0.927\n",
      "            81          -0.14026        0.927\n",
      "            82          -0.13980        0.927\n",
      "            83          -0.13934        0.927\n",
      "            84          -0.13890        0.927\n",
      "            85          -0.13846        0.927\n",
      "            86          -0.13804        0.927\n",
      "            87          -0.13762        0.927\n",
      "            88          -0.13721        0.927\n",
      "            89          -0.13681        0.927\n",
      "            90          -0.13642        0.927\n",
      "            91          -0.13604        0.927\n",
      "            92          -0.13567        0.927\n",
      "            93          -0.13530        0.927\n",
      "            94          -0.13494        0.927\n",
      "            95          -0.13459        0.927\n",
      "            96          -0.13424        0.927\n",
      "            97          -0.13390        0.927\n",
      "            98          -0.13357        0.927\n",
      "            99          -0.13324        0.927\n",
      "         Final          -0.13292        0.927\n"
     ]
    }
   ],
   "source": [
    "## Black\n",
    "black_tokens = makeTokens(['West European', 'Continental African', 'Muslim'], train_df_w)\n",
    "black_classifier = mxec(black_tokens)\n",
    "black_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.333\n",
      "             2          -0.62121        0.862\n",
      "             3          -0.46739        0.880\n",
      "             4          -0.39155        0.895\n",
      "             5          -0.34462        0.904\n",
      "             6          -0.31182        0.914\n",
      "             7          -0.28712        0.922\n",
      "             8          -0.26758        0.927\n",
      "             9          -0.25159        0.932\n",
      "            10          -0.23817        0.937\n",
      "            11          -0.22669        0.940\n",
      "            12          -0.21672        0.942\n",
      "            13          -0.20795        0.944\n",
      "            14          -0.20017        0.946\n",
      "            15          -0.19320        0.947\n",
      "            16          -0.18692        0.948\n",
      "            17          -0.18122        0.949\n",
      "            18          -0.17603        0.950\n",
      "            19          -0.17128        0.951\n",
      "            20          -0.16690        0.951\n",
      "            21          -0.16287        0.952\n",
      "            22          -0.15913        0.952\n",
      "            23          -0.15565        0.953\n",
      "            24          -0.15241        0.953\n",
      "            25          -0.14939        0.954\n",
      "            26          -0.14656        0.954\n",
      "            27          -0.14390        0.954\n",
      "            28          -0.14140        0.954\n",
      "            29          -0.13905        0.954\n",
      "            30          -0.13683        0.955\n",
      "            31          -0.13473        0.955\n",
      "            32          -0.13274        0.955\n",
      "            33          -0.13085        0.955\n",
      "            34          -0.12906        0.955\n",
      "            35          -0.12736        0.955\n",
      "            36          -0.12574        0.955\n",
      "            37          -0.12419        0.955\n",
      "            38          -0.12272        0.955\n",
      "            39          -0.12131        0.955\n",
      "            40          -0.11996        0.955\n",
      "            41          -0.11867        0.955\n",
      "            42          -0.11743        0.955\n",
      "            43          -0.11625        0.955\n",
      "            44          -0.11511        0.955\n",
      "            45          -0.11402        0.955\n",
      "            46          -0.11297        0.955\n",
      "            47          -0.11196        0.955\n",
      "            48          -0.11098        0.955\n",
      "            49          -0.11004        0.955\n",
      "            50          -0.10914        0.955\n",
      "            51          -0.10827        0.955\n",
      "            52          -0.10742        0.955\n",
      "            53          -0.10661        0.955\n",
      "            54          -0.10582        0.955\n",
      "            55          -0.10506        0.955\n",
      "            56          -0.10433        0.955\n",
      "            57          -0.10361        0.955\n",
      "            58          -0.10292        0.955\n",
      "            59          -0.10225        0.955\n",
      "            60          -0.10160        0.955\n",
      "            61          -0.10097        0.956\n",
      "            62          -0.10036        0.956\n",
      "            63          -0.09977        0.956\n",
      "            64          -0.09919        0.956\n",
      "            65          -0.09863        0.956\n",
      "            66          -0.09809        0.956\n",
      "            67          -0.09756        0.956\n",
      "            68          -0.09704        0.956\n",
      "            69          -0.09654        0.956\n",
      "            70          -0.09605        0.956\n",
      "            71          -0.09558        0.956\n",
      "            72          -0.09511        0.956\n",
      "            73          -0.09466        0.956\n",
      "            74          -0.09422        0.956\n",
      "            75          -0.09379        0.956\n",
      "            76          -0.09337        0.956\n",
      "            77          -0.09296        0.956\n",
      "            78          -0.09257        0.956\n",
      "            79          -0.09218        0.956\n",
      "            80          -0.09180        0.956\n",
      "            81          -0.09142        0.956\n",
      "            82          -0.09106        0.956\n",
      "            83          -0.09070        0.956\n",
      "            84          -0.09036        0.956\n",
      "            85          -0.09002        0.956\n",
      "            86          -0.08969        0.956\n",
      "            87          -0.08936        0.956\n",
      "            88          -0.08904        0.956\n",
      "            89          -0.08873        0.956\n",
      "            90          -0.08842        0.956\n",
      "            91          -0.08813        0.956\n",
      "            92          -0.08783        0.956\n",
      "            93          -0.08755        0.956\n",
      "            94          -0.08727        0.956\n",
      "            95          -0.08699        0.956\n",
      "            96          -0.08672        0.956\n",
      "            97          -0.08646        0.956\n",
      "            98          -0.08620        0.956\n",
      "            99          -0.08594        0.956\n",
      "         Final          -0.08569        0.956\n"
     ]
    }
   ],
   "source": [
    "## Asian\n",
    "asian_tokens = makeTokens(['East Asian', 'Indian', 'Muslim'], train_df_w)\n",
    "asian_classifier = mxec(asian_tokens)\n",
    "asian_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hispanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Native American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael     "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bffe41f1ccda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Michael     \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Michael'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Roberto     \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Roberto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Lee         \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lee'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Humza \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Humza'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Test Classifier\n",
    "print \"Michael     \", classifier.classify('Michael')\n",
    "print \"Roberto     \", classifier.classify('Roberto')\n",
    "print \"Lee         \", classifier.classify('Lee')\n",
    "print \"Humza \", classifier.classify('Humza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
