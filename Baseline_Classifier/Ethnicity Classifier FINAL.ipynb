{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline Ethnicity Imputer (1.0)\n",
    "\n",
    "\n",
    "** Model: ** We used a *Multinomial Logistic Regression* because:\n",
    "1. Our decision is categorical (non-binary)\n",
    "2. We can use the substrings within names as features, and let the classifier assign coefficients to them as they get correlated to ethnicities\n",
    "3. (Most Important) It allows us to output a score for each prediction, so we can tweak the threshold at which we go ahead and make a prediction. \n",
    "\n",
    "** Name-Ethnicity Datasets: ** We experiemented with two name-ethnicity datasets:\n",
    "1. A list of baby names found on FamilyEducation.com\n",
    "2. Names scraped from wikipedia that had ethnicity meta-data associated with them, open-sourced by (Ambekar, et al., 2009)\n",
    "\n",
    "** Test Set: ** Here we had to get creative. Because we didn't have actual ethnicities attached to the Voting Records, we needed to test externally. To make our test set, we:\n",
    "1. Took 50% of the total names in the Baby Names name-ethnicity dataset, and set them aside for testing\n",
    "2. Out of that dataset, we eliminated the names that appeared already in the training set. This makes our test set actually more stringent than the Voter Records set. This was necesary because: many of the names in the Voter Records were names that did not appear in our training set, so we didn't want to have an artificially high accuracy score in case names in the training set had a high propensity to re-appear within the training/test set.\n",
    "3. Used over-sampling to balance the test set.\n",
    "\n",
    "** Training Set: ** The remaining 50% names went to train our model. We over-sampled / under-sampled certain ethnicities to balance the training sets. We did not eliminate the repetition of names, because their frequencies are important because they correlate to real world frequencies.\n",
    "\n",
    "\n",
    "Chong, D., & Kim, D. (2006). The experiences and effects of economic status among racial and ethnic minorities. American Political Science Review, 100(3), 335â€“351.\n",
    "\n",
    "Ambekar, A., Ward, C., Mohammed, J., Male, S., & Skiena, S. (2009, June). Name-ethnicity classification from open sources. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge Discovery and Data Mining (pp. 49-58). ACM.\n",
    "\n",
    "** TOC **\n",
    "\n",
    "Baseline Model\n",
    "\n",
    "1. Import & Clean Name Data\n",
    "2. EDA on Name Data\n",
    "3. Training the Baseline Model\n",
    "4. Evaluating the Baseline Model\n",
    "\n",
    "Revised Model (uses Race as a Prior + bigger Wikipedia dataset)\n",
    "\n",
    "1. Import & Clean Name Data\n",
    "2. EDA on Name Data\n",
    "3. Training the Baseline Model\n",
    "4. Evaluating the Baseline Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ethnicityguesser.NLTKMaxentEthnicityClassifier import NLTKMaxentEthnicityClassifier as mxec\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import & Clean Name Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import names paired with ethnicities ##\n",
    "\n",
    "# find names of files\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"ethnicityguesser/pickled_names\"):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "# list types of ethnicities\n",
    "ethnicities = []\n",
    "for each in f:\n",
    "    ethnicities.append(each.partition('.')[0])\n",
    "\n",
    "\n",
    "# pair type of ethnicity to its names in a dict\n",
    "eth_dict = {}\n",
    "for ethnicity in ethnicities:\n",
    "    with open('ethnicityguesser/pickled_names/'+ethnicity+'.pkl', 'rb') as filename:\n",
    "        names = pickle.load(filename)\n",
    "    eth_dict[ethnicity] = names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese',\n",
       " 'vietnamese',\n",
       " 'irish',\n",
       " 'danish',\n",
       " 'french',\n",
       " 'russian',\n",
       " 'japanese',\n",
       " 'german',\n",
       " 'czech',\n",
       " 'arabic',\n",
       " 'ukranian',\n",
       " 'swedish',\n",
       " 'spanish',\n",
       " 'african',\n",
       " 'swiss',\n",
       " 'korean',\n",
       " 'jewish',\n",
       " 'greek',\n",
       " 'italian',\n",
       " 'slavic',\n",
       " 'indian',\n",
       " 'muslim',\n",
       " 'portugese']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a super list of names and true ethnicities\n",
    "\n",
    "super_list_names = []\n",
    "super_list_ethnicities = []\n",
    "\n",
    "for ethnicity in ethnicities:\n",
    "    name_list = eth_dict[ethnicity][0]\n",
    "    eth_list = []\n",
    "    for name in name_list:\n",
    "        eth_list.append(ethnicity)\n",
    "    super_list_names = super_list_names + name_list\n",
    "    super_list_ethnicities = super_list_ethnicities + eth_list\n",
    "    \n",
    "df = pd.DataFrame(\n",
    "            {'Name': super_list_names,\n",
    "             'True Ethnicity': super_list_ethnicities\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Name Data\n",
    "\n",
    "Let's examine what our name data looks like in reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9458</th>\n",
       "      <td>Grinberg</td>\n",
       "      <td>swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>Herda</td>\n",
       "      <td>czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19557</th>\n",
       "      <td>Caro</td>\n",
       "      <td>portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13215</th>\n",
       "      <td>Guell</td>\n",
       "      <td>swiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>Grunder</td>\n",
       "      <td>swiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Bonet</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13994</th>\n",
       "      <td>Awerbuch</td>\n",
       "      <td>jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17392</th>\n",
       "      <td>Agli</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11597</th>\n",
       "      <td>Mejias</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>Chabot</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name True Ethnicity\n",
       "9458   Grinberg        swedish\n",
       "7489      Herda          czech\n",
       "19557      Caro      portugese\n",
       "13215     Guell          swiss\n",
       "13208   Grunder          swiss\n",
       "2000      Bonet         french\n",
       "13994  Awerbuch         jewish\n",
       "17392      Agli        italian\n",
       "11597    Mejias        spanish\n",
       "2349     Chabot         french"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the data we have for every ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n for each ethnicity sample\n",
      "426 chinese\n",
      "129 vietnamese\n",
      "318 irish\n",
      "656 danish\n",
      "4143 french\n",
      "181 russian\n",
      "566 japanese\n",
      "723 german\n",
      "1406 czech\n",
      "108 arabic\n",
      "409 ukranian\n",
      "1158 swedish\n",
      "2366 spanish\n",
      "300 african\n",
      "774 swiss\n",
      "169 korean\n",
      "3076 jewish\n",
      "429 greek\n",
      "711 italian\n",
      "258 slavic\n",
      "580 indian\n",
      "525 muslim\n",
      "834 portugese\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"n for each ethnicity sample\"\n",
    "for ethnicity in ethnicities:\n",
    "    print len(df[df['True Ethnicity']==ethnicity]), ethnicity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing between 23 specific ethnicities accurately is more difficult than choosing between around 5 or 6 broad defined ethnicities because (1) having more choices to choose from in general creates more opportunities for classification errors, and (2) some ethnicities from similar parts of the world have overlapping names (like \"Alexander\" is a common Danish, Greek, and French name).\n",
    "\n",
    "Let's consolidate some groups that share name/cultural similarities. Let's also make equal sample sizes for each consolidated group, drawing evenly from each subgroup to prevent our classifier from ignoring \"low frequency\" ethnicities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "333\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "temp_df = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "\n",
    "target_df = temp_df\n",
    "eth_list = ['danish', 'french', 'italian']\n",
    "\n",
    "sample_df = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "for ethnicity in eth_list:\n",
    "    eth_df = df[df['True Ethnicity']==ethnicity]\n",
    "    n_per_eth = (1000 / len(eth_list))\n",
    "    print n_per_eth\n",
    "    sample_df = pd.concat([sample_df, eth_df.sample(n=n_per_eth)])\n",
    "\n",
    "target_df = pd.concat([target_df,sample_df]) \n",
    "\n",
    "target_df['True Ethnicity'] = \"white\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese',\n",
       " 'vietnamese',\n",
       " 'irish',\n",
       " 'danish',\n",
       " 'french',\n",
       " 'russian',\n",
       " 'japanese',\n",
       " 'german',\n",
       " 'czech',\n",
       " 'arabic',\n",
       " 'ukranian',\n",
       " 'swedish',\n",
       " 'spanish',\n",
       " 'african',\n",
       " 'swiss',\n",
       " 'korean',\n",
       " 'jewish',\n",
       " 'greek',\n",
       " 'italian',\n",
       " 'slavic',\n",
       " 'indian',\n",
       " 'muslim',\n",
       " 'portugese']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample size: 7994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18894</th>\n",
       "      <td>Abdi</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10846</th>\n",
       "      <td>Cobos</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637</th>\n",
       "      <td>Vidal</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11767</th>\n",
       "      <td>Napoleon</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>Adanna</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18334</th>\n",
       "      <td>Bal</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18730</th>\n",
       "      <td>Rampersaud</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18520</th>\n",
       "      <td>Jhaveri</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18948</th>\n",
       "      <td>Assaf</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8607</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name True Ethnicity\n",
       "18894        Abdi  Muslim/Arabic\n",
       "10846       Cobos       Hispanic\n",
       "16637       Vidal         Jewish\n",
       "11767    Napoleon       Hispanic\n",
       "12599      Adanna        African\n",
       "18334         Bal         Indian\n",
       "18730  Rampersaud         Indian\n",
       "18520     Jhaveri         Indian\n",
       "18948       Assaf  Muslim/Arabic\n",
       "8607        Koury  Muslim/Arabic"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consolidation function\n",
    "\n",
    "df_c = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "\n",
    "def consolidate(eth_list, target_df, consolidated_eth):\n",
    "    sample_df = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "    for ethnicity in eth_list:\n",
    "        eth_df = df[df['True Ethnicity']==ethnicity]\n",
    "        n_per_eth = (1000 / len(eth_list))\n",
    "        sample_df = pd.concat([sample_df, eth_df.sample(n=n_per_eth, replace = True)])\n",
    "    sample_df['True Ethnicity'] = consolidated_eth\n",
    "    return pd.concat([target_df,sample_df]) \n",
    "\n",
    "\n",
    "# Consolidate East European\n",
    "east_euro = ['russian','ukranian','czech','slavic']\n",
    "df_c = consolidate(east_euro, df_c, 'Eastern European')\n",
    "\n",
    "# Consolidate West European\n",
    "west_euro = ['italian','irish','danish','french',\n",
    "                'swedish','german','swiss']\n",
    "df_c = consolidate(west_euro, df_c, 'Western European')\n",
    "\n",
    "# Consolidate Muslim / Arab\n",
    "muslim_arabic = ['muslim', 'arabic']\n",
    "df_c = consolidate(muslim_arabic, df_c, 'Muslim/Arabic')\n",
    "\n",
    "# Consolidate East Asian\n",
    "east_asian = ['chinese','japanese','vietnamese','korean']\n",
    "df_c = consolidate(east_asian, df_c, 'East Asian')\n",
    "\n",
    "# Spanish / Hispanic can remain its own category\n",
    "hispanic = ['spanish','portugese'] \n",
    "df_c = consolidate(hispanic, df_c, 'Hispanic')\n",
    "\n",
    "# Jewish can remain its own category\n",
    "jewish = ['jewish']\n",
    "df_c = consolidate(jewish, df_c, 'Jewish')\n",
    "\n",
    "# Indian can remain its own category\n",
    "indian = ['indian']\n",
    "df_c = consolidate(indian, df_c, 'Indian')\n",
    "\n",
    "# African can remain its own category \n",
    "african = ['african']\n",
    "df_c = consolidate(african, df_c, 'African')\n",
    "\n",
    "print 'Cleaned sample size:', len(df_c)\n",
    "df_c.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## List Consolidated Ethnicities\n",
    "ethnicities_c = [\n",
    "    'Eastern European',\n",
    "    'Western European',\n",
    "    'Muslim/Arabic',\n",
    "    'East Asian',\n",
    "    'Hispanic',\n",
    "    'Jewish',\n",
    "    'Indian',\n",
    "    'African'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample (n) 7994\n",
      "Test Sample (test n) 3894\n",
      "Train Sample (train n) 4100\n"
     ]
    }
   ],
   "source": [
    "## Split into Training and Test\n",
    "msk = np.random.rand(len(df_c)) < 0.5\n",
    "train_df = df_c[msk]\n",
    "test_df = df_c[~msk]\n",
    "\n",
    "print \"Total Sample (n)\", len (df_c)\n",
    "print \"Test Sample (test n)\", len(test_df)\n",
    "print \"Train Sample (train n)\", len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Package DF into training token\n",
    "train_tokens = []\n",
    "for ethnicity in ethnicities_c:\n",
    "    new_tokens = (list(train_df[train_df['True Ethnicity'] == ethnicity]['Name']), ethnicity)\n",
    "    train_tokens.append(new_tokens)\n",
    "\n",
    "# (Tokens must be a list of ([list of names], 'ethnicity') pairs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.07944        0.122\n",
      "             2          -1.14179        0.929\n",
      "             3          -0.78956        0.950\n",
      "             4          -0.61208        0.966\n",
      "             5          -0.50362        0.977\n",
      "             6          -0.42959        0.982\n",
      "             7          -0.37544        0.986\n",
      "             8          -0.33392        0.988\n",
      "             9          -0.30099        0.990\n",
      "            10          -0.27420        0.990\n",
      "            11          -0.25196        0.991\n",
      "            12          -0.23319        0.991\n",
      "            13          -0.21713        0.991\n",
      "            14          -0.20324        0.991\n",
      "            15          -0.19109        0.991\n",
      "            16          -0.18039        0.992\n",
      "            17          -0.17088        0.992\n",
      "            18          -0.16238        0.992\n",
      "            19          -0.15473        0.992\n",
      "            20          -0.14781        0.992\n",
      "            21          -0.14153        0.992\n",
      "            22          -0.13579        0.992\n",
      "            23          -0.13054        0.992\n",
      "            24          -0.12570        0.992\n",
      "            25          -0.12124        0.992\n",
      "            26          -0.11711        0.992\n",
      "            27          -0.11328        0.992\n",
      "            28          -0.10971        0.992\n",
      "            29          -0.10638        0.992\n",
      "            30          -0.10326        0.992\n",
      "            31          -0.10034        0.992\n",
      "            32          -0.09760        0.992\n",
      "            33          -0.09502        0.992\n",
      "            34          -0.09258        0.992\n",
      "            35          -0.09028        0.992\n",
      "            36          -0.08811        0.992\n",
      "            37          -0.08605        0.992\n",
      "            38          -0.08410        0.992\n",
      "            39          -0.08224        0.992\n",
      "            40          -0.08048        0.992\n",
      "            41          -0.07879        0.992\n",
      "            42          -0.07719        0.992\n",
      "            43          -0.07566        0.992\n",
      "            44          -0.07420        0.992\n",
      "            45          -0.07280        0.992\n",
      "            46          -0.07146        0.992\n",
      "            47          -0.07018        0.992\n",
      "            48          -0.06895        0.992\n",
      "            49          -0.06777        0.992\n",
      "            50          -0.06663        0.992\n",
      "            51          -0.06554        0.992\n",
      "            52          -0.06449        0.992\n",
      "            53          -0.06348        0.992\n",
      "            54          -0.06250        0.992\n",
      "            55          -0.06156        0.992\n",
      "            56          -0.06065        0.992\n",
      "            57          -0.05978        0.992\n",
      "            58          -0.05893        0.992\n",
      "            59          -0.05811        0.992\n",
      "            60          -0.05732        0.992\n",
      "            61          -0.05656        0.992\n",
      "            62          -0.05581        0.992\n",
      "            63          -0.05510        0.992\n",
      "            64          -0.05440        0.992\n",
      "            65          -0.05372        0.992\n",
      "            66          -0.05307        0.992\n",
      "            67          -0.05243        0.992\n",
      "            68          -0.05181        0.992\n",
      "            69          -0.05121        0.992\n",
      "            70          -0.05063        0.992\n",
      "            71          -0.05006        0.992\n",
      "            72          -0.04951        0.992\n",
      "            73          -0.04898        0.992\n",
      "            74          -0.04845        0.992\n",
      "            75          -0.04795        0.992\n",
      "            76          -0.04745        0.992\n",
      "            77          -0.04697        0.992\n",
      "            78          -0.04650        0.992\n",
      "            79          -0.04604        0.992\n",
      "            80          -0.04559        0.992\n",
      "            81          -0.04515        0.992\n",
      "            82          -0.04473        0.992\n",
      "            83          -0.04431        0.992\n",
      "            84          -0.04390        0.992\n",
      "            85          -0.04351        0.992\n",
      "            86          -0.04312        0.992\n",
      "            87          -0.04274        0.992\n",
      "            88          -0.04237        0.992\n",
      "            89          -0.04201        0.992\n",
      "            90          -0.04165        0.992\n",
      "            91          -0.04131        0.992\n",
      "            92          -0.04097        0.992\n",
      "            93          -0.04063        0.992\n",
      "            94          -0.04031        0.992\n",
      "            95          -0.03999        0.992\n",
      "            96          -0.03968        0.992\n",
      "            97          -0.03937        0.992\n",
      "            98          -0.03907        0.992\n",
      "            99          -0.03878        0.992\n",
      "         Final          -0.03849        0.992\n"
     ]
    }
   ],
   "source": [
    "## Train Classifier (beware, this takes time)\n",
    "\n",
    "classifier = mxec(train_tokens)\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western European\n",
      "Hispanic\n",
      "East Asian\n",
      "Muslim/Arabic\n",
      "[('Eastern European', -3.7065529482840653), ('Jewish', -6.6514402039659508), ('Western European', -0.16855233603485154), ('African', -11.308063175888986), ('Hispanic', -7.7907496492955053), ('Indian', -8.3545578338112048), ('East Asian', -6.0727470332155962), ('Muslim/Arabic', -10.121878347922477)]\n"
     ]
    }
   ],
   "source": [
    "# Test Classifier\n",
    "print classifier.classify('Michael')\n",
    "print classifier.classify('Roberto')\n",
    "print classifier.classify('Lee')\n",
    "print classifier.classify('sajkfldsafh')\n",
    "\n",
    "def prob(name):\n",
    "    return classifier.prob_classify(name)._prob_dict.items()\n",
    "    #return max((p,v) for (v,p) in classifier.prob_classify(name)._prob_dict.items())\n",
    "\n",
    "# find probability of prediction as log (lower is better)\n",
    "print prob('Michael')\n",
    "#print prob('Roberto')\n",
    "#print prob('Lee')\n",
    "#print prob('sajkfldsafh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Triebel</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>Hakimi</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>Alpron</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Apollo</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>Baria</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>Shankar</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>Zimbalist</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>Mifsud</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>Muraoka</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Acconci</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Chernoff</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Petrik</td>\n",
       "      <td>Eastern European</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Magro</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>Kwabena</td>\n",
       "      <td>African</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Mccormick</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name        Prediction    True Ethnicity\n",
       "419     Triebel  Western European  Eastern European\n",
       "1386     Hakimi     Muslim/Arabic     Muslim/Arabic\n",
       "2639     Alpron  Western European            Jewish\n",
       "1978     Apollo          Hispanic          Hispanic\n",
       "2899      Baria          Hispanic            Indian\n",
       "3379    Shankar            Indian            Indian\n",
       "2741  Zimbalist            Jewish            Jewish\n",
       "1314     Mifsud     Muslim/Arabic     Muslim/Arabic\n",
       "1589    Muraoka        East Asian        East Asian\n",
       "479     Acconci  Western European  Western European\n",
       "99     Chernoff            Jewish  Eastern European\n",
       "172      Petrik  Eastern European  Eastern European\n",
       "1955      Magro          Hispanic          Hispanic\n",
       "3786    Kwabena           African           African\n",
       "563   Mccormick  Western European  Western European"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict!!!!\n",
    "\n",
    "test_names = list(test_df['Name'])\n",
    "test_eth = list(test_df['True Ethnicity'])\n",
    "\n",
    "test_preds = []\n",
    "\n",
    "for name in test_names:\n",
    "    pred = classifier.classify(name)\n",
    "    test_preds.append(pred)\n",
    "\n",
    "df_preds = pd.DataFrame({\n",
    "    'Name': test_names,\n",
    "    'True Ethnicity': test_eth,\n",
    "    'Prediction': test_preds\n",
    "})\n",
    "\n",
    "df_preds.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>De Araujo</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>Siddiqui</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Tzarfat</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>Chazzan</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Urban</td>\n",
       "      <td>Eastern European</td>\n",
       "      <td>Eastern European</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>Atiyeh</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>Upadhyay</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>Sundaram</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Oluwatoyin</td>\n",
       "      <td>African</td>\n",
       "      <td>African</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>Kohen</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Kim</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>Marcelino</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>Velez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>Ray</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name        Prediction    True Ethnicity  Accuracy\n",
       "1368       Koury     Muslim/Arabic     Muslim/Arabic      True\n",
       "2245   De Araujo          Hispanic          Hispanic      True\n",
       "1092    Siddiqui     Muslim/Arabic     Muslim/Arabic      True\n",
       "2399     Tzarfat            Jewish            Jewish      True\n",
       "2872     Chazzan            Jewish            Jewish      True\n",
       "113        Urban  Eastern European  Eastern European      True\n",
       "1345      Atiyeh     Muslim/Arabic     Muslim/Arabic      True\n",
       "3146    Upadhyay            Indian            Indian      True\n",
       "3215    Sundaram            Jewish            Indian     False\n",
       "3801  Oluwatoyin           African           African      True\n",
       "2625       Kohen            Jewish            Jewish      True\n",
       "1862         Kim     Muslim/Arabic        East Asian     False\n",
       "2308   Marcelino          Hispanic          Hispanic      True\n",
       "2249       Velez          Hispanic          Hispanic      True\n",
       "3090         Ray            Indian            Indian      True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add True if you got it right\n",
    "df_preds['Accuracy'] = (df_preds['Prediction']==df_preds['True Ethnicity'])\n",
    "df_preds.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tools to Calculate One vs. Rest Accuracy Rates\n",
    "\n",
    "def calcTP(df, eth):\n",
    "    P = df[df['Prediction']==eth]\n",
    "    TP = P[P['True Ethnicity']==eth]\n",
    "    return len(TP)\n",
    "\n",
    "def calcFP(df, eth):\n",
    "    P = df[df['Prediction']==eth]\n",
    "    FP = P[P['True Ethnicity']!=eth]\n",
    "    return len(FP)\n",
    "\n",
    "def calcTN(df, eth):\n",
    "    N = df[df['Prediction']!=eth]\n",
    "    TN = N[N['True Ethnicity']!=eth]\n",
    "    return len(TN)\n",
    "\n",
    "def calcFN(df, eth):\n",
    "    N = df[df['Prediction']!=eth]\n",
    "    FN = N[N['True Ethnicity']==eth]\n",
    "    return len(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TPs = [] # number of times predict X when  X ethnicity\n",
    "FPs = [] # number of times predict X when 'X ethnicity\n",
    "TNs = [] # number of times predict'X when 'X ethnicity\n",
    "FNs = [] # number of times predict'X when  X ethnicity\n",
    "\n",
    "\n",
    "ethnicity_list = []\n",
    "\n",
    "# Classification Accuracy\n",
    "for ethnicity in ethnicities_c:\n",
    "\n",
    "    TPs.append(calcTP(df_preds, ethnicity))\n",
    "    FPs.append(calcFP(df_preds, ethnicity))\n",
    "    TNs.append(calcTN(df_preds, ethnicity))\n",
    "    FNs.append(calcFN(df_preds, ethnicity))\n",
    "    ethnicity_list.append(ethnicity)\n",
    "\n",
    "\n",
    "\n",
    "# put into df\n",
    "df_acc = pd.DataFrame({\n",
    "    'True Ethnicity': ethnicity_list,\n",
    "    'TP': TPs,\n",
    "    'FP': FPs,\n",
    "    'TN': TNs,\n",
    "    'FN': FNs\n",
    "})\n",
    "\n",
    "df_acc.set_index('True Ethnicity', inplace=True)\n",
    "\n",
    "# Add TPR (Sensistivity)\n",
    "df_acc['Sensitivity (TPR)'] = (df_acc['TP']) / (df_acc['TP'] + df_acc['FN'])\n",
    "\n",
    "# Add FPR\n",
    "df_acc['FPR'] = (df_acc['FP']) / (df_acc['FP'] + df_acc['TN'])\n",
    "\n",
    "# Add Precision\n",
    "df_acc['Precision'] = (df_acc['TP']) / (df_acc['TP'] + df_acc['FP'])\n",
    "\n",
    "# F1 Score (harmonic mean of precision and sensitivity)\n",
    "df_acc['F1 Score'] = (2 * df_acc['TP'] / ((2*df_acc['TP'])+df_acc['FP']+df_acc['FN']))\n",
    "\n",
    "# Accuracy (ACC)\n",
    "df_acc['ACC'] = (df_acc['TP']+df_acc['TN']) / (df_acc['TP']+df_acc['TN']+df_acc['FP']+df_acc['FN'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Accuracy</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity (TPR)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eastern European</th>\n",
       "      <td>0.737418</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>3307</td>\n",
       "      <td>337</td>\n",
       "      <td>0.737418</td>\n",
       "      <td>0.037824</td>\n",
       "      <td>0.721627</td>\n",
       "      <td>0.729437</td>\n",
       "      <td>0.935799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western European</th>\n",
       "      <td>0.577236</td>\n",
       "      <td>208</td>\n",
       "      <td>174</td>\n",
       "      <td>3228</td>\n",
       "      <td>284</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.051146</td>\n",
       "      <td>0.620087</td>\n",
       "      <td>0.597895</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Muslim/Arabic</th>\n",
       "      <td>0.852590</td>\n",
       "      <td>74</td>\n",
       "      <td>123</td>\n",
       "      <td>3269</td>\n",
       "      <td>428</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.776770</td>\n",
       "      <td>0.812915</td>\n",
       "      <td>0.949409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>East Asian</th>\n",
       "      <td>0.789130</td>\n",
       "      <td>97</td>\n",
       "      <td>96</td>\n",
       "      <td>3338</td>\n",
       "      <td>363</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.790850</td>\n",
       "      <td>0.789989</td>\n",
       "      <td>0.950437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>0.776639</td>\n",
       "      <td>109</td>\n",
       "      <td>138</td>\n",
       "      <td>3268</td>\n",
       "      <td>379</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.040517</td>\n",
       "      <td>0.733075</td>\n",
       "      <td>0.754229</td>\n",
       "      <td>0.936569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewish</th>\n",
       "      <td>0.520243</td>\n",
       "      <td>237</td>\n",
       "      <td>160</td>\n",
       "      <td>3240</td>\n",
       "      <td>257</td>\n",
       "      <td>0.520243</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.616307</td>\n",
       "      <td>0.564215</td>\n",
       "      <td>0.898048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.760784</td>\n",
       "      <td>122</td>\n",
       "      <td>113</td>\n",
       "      <td>3271</td>\n",
       "      <td>388</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>0.774451</td>\n",
       "      <td>0.767557</td>\n",
       "      <td>0.939651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.947047</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>3344</td>\n",
       "      <td>465</td>\n",
       "      <td>0.947047</td>\n",
       "      <td>0.017338</td>\n",
       "      <td>0.887405</td>\n",
       "      <td>0.916256</td>\n",
       "      <td>0.978172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Classification Accuracy   FN   FP    TN   TP  \\\n",
       "True Ethnicity                                                   \n",
       "Eastern European                 0.737418  120  130  3307  337   \n",
       "Western European                 0.577236  208  174  3228  284   \n",
       "Muslim/Arabic                    0.852590   74  123  3269  428   \n",
       "East Asian                       0.789130   97   96  3338  363   \n",
       "Hispanic                         0.776639  109  138  3268  379   \n",
       "Jewish                           0.520243  237  160  3240  257   \n",
       "Indian                           0.760784  122  113  3271  388   \n",
       "African                          0.947047   26   59  3344  465   \n",
       "\n",
       "                  Sensitivity (TPR)       FPR  Precision  F1 Score       ACC  \n",
       "True Ethnicity                                                                \n",
       "Eastern European           0.737418  0.037824   0.721627  0.729437  0.935799  \n",
       "Western European           0.577236  0.051146   0.620087  0.597895  0.901900  \n",
       "Muslim/Arabic              0.852590  0.036262   0.776770  0.812915  0.949409  \n",
       "East Asian                 0.789130  0.027956   0.790850  0.789989  0.950437  \n",
       "Hispanic                   0.776639  0.040517   0.733075  0.754229  0.936569  \n",
       "Jewish                     0.520243  0.047059   0.616307  0.564215  0.898048  \n",
       "Indian                     0.760784  0.033392   0.774451  0.767557  0.939651  \n",
       "African                    0.947047  0.017338   0.887405  0.916256  0.978172  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Our ACCs (Overall accuracy scores) are super high, and our FPRs are very low, but these are actually misleading because they are inflated by our high True Negative count. We are using a one vs. rest calculation approach. This calculation approach leads to a high number of true negatives, because most guesses will simply not be the \"one\" ethnicity in question.\n",
    "\n",
    "The most useful metrics here are probably True Positive Rate/Sensitivity (because it tells us how often we are predicting correctly within a given ethnicity), and the Precision (because it tells the likelihood of whether or not our guess is correct, once we make it).\n",
    "\n",
    "F1 Score is interesting because it gives us a harmonic mean of these two metrics (Precision an Sensitivity) so it can help us consider those two together.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "\n",
    "** Tuning this Model **\n",
    "Moving forward, we think that it would be beneficial to optimize for Precision (by only classifying as an ethnicity when we are sure, potentially abstaining in ambiguous cases). This would cause us only predict on some data points.\n",
    "\n",
    "In our research project, we think this is a good trade off. Given the enormous size of our dataset (>10 million voters in Florida), it is fine for us to abstain from predicting for many people. Even if we predict on only 1 million of the most ethnically identifiable names for each ethnicity, that is enough to make conclusions on the state level.\n",
    "\n",
    "One concern would be that be only classifying highly identifiable names, we are introducing a bias, because people of a certain ethnicity who have a less identifiable name may behave differently as voters. As a working assumption, we'll assume that the ethnic identifiability of person's name does not have a causal or confounding effect on their voting behavior, and that abstaining on those less identifiable names is better than making an incorrect classification.\n",
    "\n",
    "We will continue to explore the literature on name/language classification and will test one or more of the above models as appropriate.\n",
    "\n",
    "\n",
    "** Using Better Data **\n",
    "\n",
    "A limitation of our approach is that a certain name may appear more frequently in one ethnicity, and less in another, but our Baby Names dataset does not account for this.\n",
    "\n",
    "We will try to experiment with other data sources to see if we can get a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually imputing ethnicity into our Voter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>district</th>\n",
       "      <th>id</th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>FIRST_NAME</th>\n",
       "      <th>zip</th>\n",
       "      <th>female</th>\n",
       "      <th>dob</th>\n",
       "      <th>regyear</th>\n",
       "      <th>party</th>\n",
       "      <th>electiondate</th>\n",
       "      <th>general</th>\n",
       "      <th>typeofvote</th>\n",
       "      <th>age</th>\n",
       "      <th>GEN16</th>\n",
       "      <th>GEN14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>5139</td>\n",
       "      <td>5139</td>\n",
       "      <td>3153570</td>\n",
       "      <td>CLL</td>\n",
       "      <td>103047739</td>\n",
       "      <td>Howard</td>\n",
       "      <td>Timothy</td>\n",
       "      <td>34117</td>\n",
       "      <td>M</td>\n",
       "      <td>1961-10-26 00:00:00</td>\n",
       "      <td>09/22/2000</td>\n",
       "      <td>REP</td>\n",
       "      <td>11/06/2012</td>\n",
       "      <td>GEN</td>\n",
       "      <td>A</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>1443</td>\n",
       "      <td>1443</td>\n",
       "      <td>1696657</td>\n",
       "      <td>CHA</td>\n",
       "      <td>102606670</td>\n",
       "      <td>Pruey</td>\n",
       "      <td>R</td>\n",
       "      <td>34224</td>\n",
       "      <td>M</td>\n",
       "      <td>1980-01-11 00:00:00</td>\n",
       "      <td>05/06/1998</td>\n",
       "      <td>NPA</td>\n",
       "      <td>11/08/2016</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>4871</td>\n",
       "      <td>4871</td>\n",
       "      <td>5669616</td>\n",
       "      <td>DUV</td>\n",
       "      <td>103293240</td>\n",
       "      <td>Moore</td>\n",
       "      <td>William</td>\n",
       "      <td>32233</td>\n",
       "      <td>M</td>\n",
       "      <td>1956-11-13 00:00:00</td>\n",
       "      <td>10/08/1990</td>\n",
       "      <td>REP</td>\n",
       "      <td>11/08/2016</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>9672</td>\n",
       "      <td>9672</td>\n",
       "      <td>7433685</td>\n",
       "      <td>DUV</td>\n",
       "      <td>103841413</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>Sue</td>\n",
       "      <td>32210</td>\n",
       "      <td>U</td>\n",
       "      <td>1969-04-06 00:00:00</td>\n",
       "      <td>01/31/1995</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/06/2012</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>1608176</td>\n",
       "      <td>CHA</td>\n",
       "      <td>102626244</td>\n",
       "      <td>Kelley</td>\n",
       "      <td>Kristine</td>\n",
       "      <td>34224</td>\n",
       "      <td>F</td>\n",
       "      <td>1966-02-24 00:00:00</td>\n",
       "      <td>11/15/2000</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/07/2006</td>\n",
       "      <td>GEN</td>\n",
       "      <td>N</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>9887</td>\n",
       "      <td>9887</td>\n",
       "      <td>6220186</td>\n",
       "      <td>DUV</td>\n",
       "      <td>103752822</td>\n",
       "      <td>Carswell</td>\n",
       "      <td>Dana</td>\n",
       "      <td>32277</td>\n",
       "      <td>F</td>\n",
       "      <td>1952-09-01 00:00:00</td>\n",
       "      <td>11/08/1983</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/04/2014</td>\n",
       "      <td>GEN</td>\n",
       "      <td>E</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>3704</td>\n",
       "      <td>3704</td>\n",
       "      <td>2768859</td>\n",
       "      <td>CLL</td>\n",
       "      <td>103016985</td>\n",
       "      <td>James</td>\n",
       "      <td>Carol</td>\n",
       "      <td>34112</td>\n",
       "      <td>F</td>\n",
       "      <td>1939-06-15 00:00:00</td>\n",
       "      <td>02/26/1998</td>\n",
       "      <td>REP</td>\n",
       "      <td>11/07/2006</td>\n",
       "      <td>GEN</td>\n",
       "      <td>E</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>5248</td>\n",
       "      <td>5248</td>\n",
       "      <td>999427</td>\n",
       "      <td>BRA</td>\n",
       "      <td>100753930</td>\n",
       "      <td>Goodman</td>\n",
       "      <td>Anessa</td>\n",
       "      <td>32058</td>\n",
       "      <td>F</td>\n",
       "      <td>1975-07-07 00:00:00</td>\n",
       "      <td>05/05/1997</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/06/2012</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>5449</td>\n",
       "      <td>5449</td>\n",
       "      <td>3971893</td>\n",
       "      <td>CLM</td>\n",
       "      <td>103179402</td>\n",
       "      <td>DRAWDY</td>\n",
       "      <td>CAROLYN</td>\n",
       "      <td>32024</td>\n",
       "      <td>F</td>\n",
       "      <td>1941-07-16 00:00:00</td>\n",
       "      <td>03/28/1968</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/04/2008</td>\n",
       "      <td>GEN</td>\n",
       "      <td>E</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1595</td>\n",
       "      <td>1595</td>\n",
       "      <td>1290058</td>\n",
       "      <td>CHA</td>\n",
       "      <td>102560007</td>\n",
       "      <td>Goldman</td>\n",
       "      <td>Jason</td>\n",
       "      <td>33954</td>\n",
       "      <td>M</td>\n",
       "      <td>1970-03-23 00:00:00</td>\n",
       "      <td>06/02/1988</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/04/2014</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1    index district         id LAST_NAME  \\\n",
       "2644        5139          5139  3153570      CLL  103047739    Howard   \n",
       "754         1443          1443  1696657      CHA  102606670     Pruey   \n",
       "2516        4871          4871  5669616      DUV  103293240     Moore   \n",
       "4975        9672          9672  7433685      DUV  103841413   Sanders   \n",
       "34            70            70  1608176      CHA  102626244    Kelley   \n",
       "5081        9887          9887  6220186      DUV  103752822  Carswell   \n",
       "1910        3704          3704  2768859      CLL  103016985     James   \n",
       "2701        5248          5248   999427      BRA  100753930   Goodman   \n",
       "2801        5449          5449  3971893      CLM  103179402    DRAWDY   \n",
       "834         1595          1595  1290058      CHA  102560007   Goldman   \n",
       "\n",
       "     FIRST_NAME    zip female                  dob     regyear party  \\\n",
       "2644    Timothy  34117      M  1961-10-26 00:00:00  09/22/2000   REP   \n",
       "754           R  34224      M  1980-01-11 00:00:00  05/06/1998   NPA   \n",
       "2516    William  32233      M  1956-11-13 00:00:00  10/08/1990   REP   \n",
       "4975        Sue  32210      U  1969-04-06 00:00:00  01/31/1995   DEM   \n",
       "34     Kristine  34224      F  1966-02-24 00:00:00  11/15/2000   DEM   \n",
       "5081       Dana  32277      F  1952-09-01 00:00:00  11/08/1983   DEM   \n",
       "1910      Carol  34112      F  1939-06-15 00:00:00  02/26/1998   REP   \n",
       "2701     Anessa  32058      F  1975-07-07 00:00:00  05/05/1997   DEM   \n",
       "2801    CAROLYN  32024      F  1941-07-16 00:00:00  03/28/1968   DEM   \n",
       "834       Jason  33954      M  1970-03-23 00:00:00  06/02/1988   DEM   \n",
       "\n",
       "     electiondate general typeofvote   age  GEN16  GEN14  \n",
       "2644   11/06/2012     GEN          A  56.0      0      0  \n",
       "754    11/08/2016     GEN          Y  37.0      1      0  \n",
       "2516   11/08/2016     GEN          Y  61.0      1      0  \n",
       "4975   11/06/2012     GEN          Y  48.0      0      0  \n",
       "34     11/07/2006     GEN          N  51.0      0      0  \n",
       "5081   11/04/2014     GEN          E  65.0      0      0  \n",
       "1910   11/07/2006     GEN          E  78.0      0      0  \n",
       "2701   11/06/2012     GEN          Y  42.0      0      0  \n",
       "2801   11/04/2008     GEN          E  76.0      0      1  \n",
       "834    11/04/2014     GEN          Y  47.0      0      0  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import data cleaned by Riddhi and Kimia\n",
    "df_voters = pd.read_csv('Milestone33.csv', sep='\\t')\n",
    "df_voters.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add predictions to voter data\n",
    "ethnicity_predictions = []\n",
    "for name in list(df_voters['LAST_NAME']):\n",
    "    ethnicity_predictions.append(classifier.classify(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_voters['Ethnicity Prediction'] = ethnicity_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>Ethnicity Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>Kelleher</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>Pullen</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>Pineda</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>MCDOWELL</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>Lessord</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>Clemons</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>Montgomery</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>THIBODAUX</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>RANDLES</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>THORNTON</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>Kopren</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>Prosper</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>BEAM</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Jackson</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Torres</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LAST_NAME Ethnicity Prediction\n",
       "1526    Kelleher               Jewish\n",
       "3536      Pullen     Western European\n",
       "4547      Pineda             Hispanic\n",
       "4707    MCDOWELL     Western European\n",
       "2959     Lessord     Eastern European\n",
       "3846     Clemons     Western European\n",
       "3054  Montgomery     Western European\n",
       "4772   THIBODAUX     Western European\n",
       "1682     RANDLES             Hispanic\n",
       "3758    THORNTON     Western European\n",
       "4916      Kopren     Western European\n",
       "1937     Prosper     Western European\n",
       "108         BEAM               Jewish\n",
       "585      Jackson               Jewish\n",
       "452       Torres             Hispanic"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voters.sample(15)[['LAST_NAME', 'Ethnicity Prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Baseline looks reasonable on the voter data, but definitely has room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Revised Ethnicity Imputer (2.0)\n",
    "\n",
    "## Improvements:\n",
    "* Bigger dataset (Wikipedia)\n",
    "* Option to abstain from predicting when uncertain\n",
    "* Ability to use \"Race\" information as a prior to eliminate unlikely ethnicities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import and clean the wikipedia name data, as it it larger and may be able to better train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_middle</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98023</th>\n",
       "      <td>paul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lyn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterEuropean,British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39842</th>\n",
       "      <td>okumu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sibi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterAfrican,Africans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58182</th>\n",
       "      <td>iv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>honorÃ©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterEuropean,WestEuropean,French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131755</th>\n",
       "      <td>rao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rao</td>\n",
       "      <td>gopal</td>\n",
       "      <td>Asian,IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96192</th>\n",
       "      <td>furphy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ken</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GreaterEuropean,British</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_last name_suffix name_first name_middle  \\\n",
       "98023       paul         NaN        lyn         NaN   \n",
       "39842      okumu         NaN       sibi         NaN   \n",
       "58182         iv         NaN     honorÃ©         NaN   \n",
       "131755       rao         NaN        rao       gopal   \n",
       "96192     furphy         NaN        ken         NaN   \n",
       "\n",
       "                                       race  \n",
       "98023               GreaterEuropean,British  \n",
       "39842               GreaterAfrican,Africans  \n",
       "58182   GreaterEuropean,WestEuropean,French  \n",
       "131755             Asian,IndianSubContinent  \n",
       "96192               GreaterEuropean,British  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_raw = pd.read_csv('wikipedia_data_scraped/wiki_name_race.csv')\n",
    "df_wiki_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yep. This is going to need some cleaning. Let's do this by:\n",
    "- Creating a new row for each name that is present (first, middle, last). We'll assume for not that the distinction is not important.\n",
    "- For ethnicity, let's only use the most specific ethnicity available to us (e.g. Italian instead of WestEuropean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_wiki_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_list_names = []\n",
    "super_list_ethnicities = []\n",
    "\n",
    "# clean names, simplify \"ethnicity\" field to just most specific one\n",
    "for row in range(len(df_wiki_raw)):\n",
    "    # filter valid first names\n",
    "    if type(df_wiki_raw.iloc[row].name_first) == str and len(df_wiki_raw.iloc[row].name_first) > 2:\n",
    "        super_list_names.append(df_wiki_raw.iloc[row].name_first)\n",
    "        super_list_ethnicities.append(df_wiki_raw.iloc[row].race.split(',')[-1])\n",
    "    # filter valid middle names\n",
    "    if type(df_wiki_raw.iloc[row].name_middle) == str and len(df_wiki_raw.iloc[row].name_middle) > 2:\n",
    "        super_list_names.append(df_wiki_raw.iloc[row].name_middle)\n",
    "        super_list_ethnicities.append(df_wiki_raw.iloc[row].race.split(',')[-1])\n",
    "    # filter valid last names\n",
    "    if type(df_wiki_raw.iloc[row].name_last) == str and len(df_wiki_raw.iloc[row].name_last) > 2:\n",
    "        super_list_names.append(df_wiki_raw.iloc[row].name_last)\n",
    "        super_list_ethnicities.append(df_wiki_raw.iloc[row].race.split(',')[-1])\n",
    "\n",
    "# throw it into a dataframe\n",
    "df_wiki = pd.DataFrame(\n",
    "            {'Name': super_list_names,\n",
    "             'True Ethnicity': super_list_ethnicities\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256259</th>\n",
       "      <td>abidi</td>\n",
       "      <td>IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206431</th>\n",
       "      <td>elmsley</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>muhammad</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252247</th>\n",
       "      <td>arun</td>\n",
       "      <td>IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195046</th>\n",
       "      <td>hunter</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104580</th>\n",
       "      <td>kuramoto</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244159</th>\n",
       "      <td>fan</td>\n",
       "      <td>EastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72702</th>\n",
       "      <td>weinberger</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154685</th>\n",
       "      <td>andrew</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169829</th>\n",
       "      <td>harley</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279857</th>\n",
       "      <td>sinimberghi</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21087</th>\n",
       "      <td>bin laden tape</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19056</th>\n",
       "      <td>nil</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241353</th>\n",
       "      <td>michitsura</td>\n",
       "      <td>EastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256468</th>\n",
       "      <td>d'cruz</td>\n",
       "      <td>IndianSubContinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275894</th>\n",
       "      <td>duchy of savoy</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215670</th>\n",
       "      <td>fanshawe</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215872</th>\n",
       "      <td>simon</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103052</th>\n",
       "      <td>hiratsuka</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60994</th>\n",
       "      <td>azaziah</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name      True Ethnicity\n",
       "256259           abidi  IndianSubContinent\n",
       "206431         elmsley             British\n",
       "18151         muhammad              Muslim\n",
       "252247            arun  IndianSubContinent\n",
       "195046          hunter             British\n",
       "104580        kuramoto            Japanese\n",
       "244159             fan           EastAsian\n",
       "72702       weinberger              Jewish\n",
       "154685          andrew             British\n",
       "169829          harley             British\n",
       "279857     sinimberghi             Italian\n",
       "21087   bin laden tape              Muslim\n",
       "19056              nil              Muslim\n",
       "241353      michitsura           EastAsian\n",
       "256468          d'cruz  IndianSubContinent\n",
       "275894  duchy of savoy             Italian\n",
       "215670        fanshawe             British\n",
       "215872           simon             British\n",
       "103052       hiratsuka            Japanese\n",
       "60994          azaziah              Jewish"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>295895</td>\n",
       "      <td>295895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>97507</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>john</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2910</td>\n",
       "      <td>88353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name True Ethnicity\n",
       "count   295895         295895\n",
       "unique   97507             13\n",
       "top       john        British\n",
       "freq      2910          88353"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's standardize the ethnicities between our two datasets (the baby names dataset has ten more ethnicity categories than the Wikipedia dataset, 23 and 13 respectively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## standardize & consolidated ethnicities\n",
    "# format = c_eth[\"consolidated\"] = [[' eth from baby names'],['eth from wiki']]\n",
    "\n",
    "ethnicities_wiki = df_wiki[\"True Ethnicity\"].unique()\n",
    "\n",
    "c_eth = {}\n",
    "\n",
    "c_eth[\"East European\"] = ['russian','ukranian','czech','slavic', 'greek', # baby names\n",
    "                          'EastEuropean'] # wiki names\n",
    "\n",
    "c_eth['West European'] = ['italian','irish','danish','french', 'swedish','german','swiss',\n",
    "                          'Nordic','British', 'Germanic', 'French', 'Italian'] \n",
    "\n",
    "c_eth['Muslim'] = ['muslim', 'arabic',\n",
    "                   'Muslim']\n",
    "\n",
    "c_eth['East Asian'] = ['chinese','japanese','vietnamese','korean',\n",
    "                       'EastAsian', 'Japanese']\n",
    "\n",
    "c_eth['Hispanic'] = ['spanish','portugese',\n",
    "                     'Hispanic']\n",
    "\n",
    "c_eth['Jewish'] = ['jewish','Jewish']\n",
    "\n",
    "c_eth['Indian'] = ['indian','IndianSubContinent']\n",
    "\n",
    "c_eth['Continental African'] = ['african','Africans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20245 20245 20245\n",
      "295895 295895 295895\n"
     ]
    }
   ],
   "source": [
    "## transform datasets\n",
    "def standardizeEth(df):\n",
    "    names = list(df['Name'])\n",
    "    org_eth = list(df['True Ethnicity'])    \n",
    "    standard_eth = []\n",
    "    for ethnicity in org_eth:\n",
    "        # search ethnicity dict\n",
    "        for c in c_eth:\n",
    "            # if found\n",
    "            if ethnicity in c_eth[c]:\n",
    "                # then add to master list\n",
    "                standard_eth.append(c)\n",
    "    print len(names), len(standard_eth), len(org_eth)\n",
    "    df_new = pd.DataFrame(\n",
    "            {'Name': names,\n",
    "             'True Ethnicity': org_eth,\n",
    "             'Standardized Ethnicity': standard_eth\n",
    "            })\n",
    "    return df_new\n",
    "\n",
    "df_baby = df\n",
    "\n",
    "df_baby_standard = standardizeEth(df_baby)\n",
    "\n",
    "df_wiki_standard = standardizeEth(df_wiki)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Standardized Ethnicity</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177607</th>\n",
       "      <td>davies</td>\n",
       "      <td>West European</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63333</th>\n",
       "      <td>rowe</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>roesler</td>\n",
       "      <td>West European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140517</th>\n",
       "      <td>patrushev</td>\n",
       "      <td>East European</td>\n",
       "      <td>EastEuropean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13045</th>\n",
       "      <td>valero</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name Standardized Ethnicity True Ethnicity\n",
       "177607     davies          West European        British\n",
       "63333        rowe                 Jewish         Jewish\n",
       "2108      roesler          West European       Germanic\n",
       "140517  patrushev          East European   EastEuropean\n",
       "13045      valero                 Muslim         Muslim"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_standard.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_wiki_standard.sample(5)## standardize & consolidated ethnicities\n",
    "# format = c_eth[\"consolidated\"] = [[' eth from baby names'],['eth from wiki']]\n",
    "\n",
    "ethnicities_wiki = df_wiki[\"True Ethnicity\"].unique()\n",
    "\n",
    "c_eth = {}\n",
    "\n",
    "c_eth[\"East European\"] = ['russian','ukranian','czech','slavic', 'greek', # baby names\n",
    "                          'EastEuropean'] # wiki names\n",
    "\n",
    "c_eth['West European'] = ['italian','irish','danish','french', 'swedish','german','swiss',\n",
    "                          'Nordic','British', 'Germanic', 'French', 'Italian'] \n",
    "\n",
    "c_eth['Muslim'] = ['muslim', 'arabic',\n",
    "                   'Muslim']\n",
    "\n",
    "c_eth['East Asian'] = ['chinese','japanese','vietnamese','korean',\n",
    "                       'EastAsian', 'Japanese']\n",
    "\n",
    "c_eth['Hispanic'] = ['spanish','portugese',\n",
    "                     'Hispanic']\n",
    "\n",
    "c_eth['Jewish'] = ['jewish','Jewish']\n",
    "\n",
    "c_eth['Indian'] = ['indian','IndianSubContinent']\n",
    "\n",
    "c_eth['Continental African'] = ['african','Africans']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, nice data that we can use. Let's do one last step and balance the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "British               88353\n",
       "French                27566\n",
       "Italian               26711\n",
       "Hispanic              24469\n",
       "Jewish                22406\n",
       "EastEuropean          18311\n",
       "IndianSubContinent    17988\n",
       "Japanese              15906\n",
       "Muslim                14340\n",
       "EastAsian             11459\n",
       "Nordic                10927\n",
       "Germanic               8999\n",
       "Africans               8460\n",
       "Name: True Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Balance Wiki\n",
    "\n",
    "df_wiki_standard['True Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spanish', 'portugese', 'Hispanic']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_eth[c_eth.keys()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_wiki_balanced = pd.DataFrame(columns=['Name', 'Standardized Ethnicity','True Ethnicity'])\n",
    "\n",
    "##  balancing #1 - balance by True Ethnicity\n",
    "for eth in ethnicities_wiki:\n",
    "    # sample maximum amount from each(8460 - limit because of Africans)\n",
    "    sample_df = df_wiki_standard[df_wiki_standard['True Ethnicity']==eth].sample(8460)\n",
    "    df_wiki_balanced = pd.concat([df_wiki_balanced,sample_df])\n",
    "    \n",
    "## balancing #2 - balance by Standardized Eth, with equal numbers of True Eth in each group\n",
    "df_wiki_b = pd.DataFrame(columns=['Name', 'Standardized Ethnicity','True Ethnicity'])\n",
    "\n",
    "for eth in c_eth.keys():\n",
    "    sample_df = df_wiki_balanced[df_wiki_balanced['Standardized Ethnicity']==eth].sample(8460)\n",
    "    df_wiki_b = pd.concat([df_wiki_b,sample_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the proportions make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "West European          8460\n",
       "East Asian             8460\n",
       "Muslim                 8460\n",
       "Indian                 8460\n",
       "East European          8460\n",
       "Continental African    8460\n",
       "Jewish                 8460\n",
       "Hispanic               8460\n",
       "Name: Standardized Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_b['Standardized Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Muslim                8460\n",
       "Africans              8460\n",
       "Jewish                8460\n",
       "Hispanic              8460\n",
       "IndianSubContinent    8460\n",
       "EastEuropean          8460\n",
       "EastAsian             4258\n",
       "Japanese              4202\n",
       "British               1735\n",
       "Germanic              1713\n",
       "Nordic                1702\n",
       "French                1659\n",
       "Italian               1651\n",
       "Name: True Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_b['True Ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, lets finish this up by splitting into test/training (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample (n) 67680\n",
      "Test Sample (test n) 13616\n",
      "Train Sample (train n) 54064\n"
     ]
    }
   ],
   "source": [
    "## Split into Training and Test\n",
    "msk = np.random.rand(len(df_wiki_b)) < 0.8\n",
    "train_df_w = df_wiki_b[msk]\n",
    "test_df_w = df_wiki_b[~msk]\n",
    "\n",
    "print \"Total Sample (n)\", len (df_wiki_b)\n",
    "print \"Test Sample (test n)\", len(test_df_w)\n",
    "print \"Train Sample (train n)\", len(train_df_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hispanic\n",
      "Jewish\n",
      "East Asian\n",
      "Muslim\n",
      "West European\n",
      "East European\n",
      "Indian\n",
      "Continental African\n"
     ]
    }
   ],
   "source": [
    "# Our ethnicities\n",
    "for each in c_eth:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the Revised Ethnicity Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets generate training tokens from our training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Package DF into training token\n",
    "\n",
    "def makeTokens(ethnicities, train_df):\n",
    "    train_tokens = []\n",
    "    for ethnicity in ethnicities:\n",
    "        new_tokens = (list(train_df[train_df['Standardized Ethnicity'] == ethnicity]['Name']), ethnicity)\n",
    "        train_tokens.append(new_tokens)\n",
    "    return train_tokens\n",
    "\n",
    "wiki_tokens = makeTokens(c_eth, train_df_w)\n",
    "\n",
    "\n",
    "# (Tokens must be a list of ([list of names], 'ethnicity') pairs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train Classifier (beware, this takes time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.38629        0.250\n",
      "             2          -0.89420        0.786\n",
      "             3          -0.70446        0.805\n",
      "             4          -0.60567        0.821\n",
      "             5          -0.54320        0.835\n",
      "             6          -0.49901        0.846\n",
      "             7          -0.46546        0.855\n",
      "             8          -0.43875        0.861\n",
      "             9          -0.41676        0.867\n",
      "            10          -0.39821        0.871\n",
      "            11          -0.38226        0.875\n",
      "            12          -0.36835        0.879\n",
      "            13          -0.35606        0.881\n",
      "            14          -0.34511        0.884\n",
      "            15          -0.33526        0.886\n",
      "            16          -0.32636        0.887\n",
      "            17          -0.31825        0.889\n",
      "            18          -0.31084        0.890\n",
      "            19          -0.30403        0.891\n",
      "            20          -0.29775        0.892\n",
      "            21          -0.29194        0.893\n",
      "            22          -0.28654        0.894\n",
      "            23          -0.28151        0.895\n",
      "            24          -0.27682        0.895\n",
      "            25          -0.27242        0.896\n",
      "            26          -0.26830        0.896\n",
      "            27          -0.26443        0.897\n",
      "            28          -0.26077        0.897\n",
      "            29          -0.25733        0.898\n",
      "            30          -0.25407        0.898\n",
      "            31          -0.25099        0.898\n",
      "            32          -0.24807        0.898\n",
      "            33          -0.24529        0.899\n",
      "            34          -0.24265        0.899\n",
      "            35          -0.24014        0.899\n",
      "            36          -0.23775        0.899\n",
      "            37          -0.23546        0.899\n",
      "            38          -0.23328        0.899\n",
      "            39          -0.23119        0.900\n",
      "            40          -0.22919        0.900\n",
      "            41          -0.22728        0.900\n",
      "            42          -0.22544        0.900\n",
      "            43          -0.22368        0.900\n",
      "            44          -0.22199        0.900\n",
      "            45          -0.22036        0.900\n",
      "            46          -0.21880        0.900\n",
      "            47          -0.21729        0.900\n",
      "            48          -0.21584        0.900\n",
      "            49          -0.21444        0.900\n",
      "            50          -0.21309        0.900\n",
      "            51          -0.21179        0.900\n",
      "            52          -0.21053        0.900\n",
      "            53          -0.20931        0.900\n",
      "            54          -0.20814        0.900\n",
      "            55          -0.20700        0.900\n",
      "            56          -0.20590        0.900\n",
      "            57          -0.20483        0.900\n",
      "            58          -0.20379        0.900\n",
      "            59          -0.20279        0.900\n",
      "            60          -0.20182        0.900\n",
      "            61          -0.20088        0.900\n",
      "            62          -0.19996        0.900\n",
      "            63          -0.19907        0.900\n",
      "            64          -0.19820        0.900\n",
      "            65          -0.19736        0.900\n",
      "            66          -0.19655        0.900\n",
      "            67          -0.19575        0.900\n",
      "            68          -0.19498        0.900\n",
      "            69          -0.19422        0.900\n",
      "            70          -0.19349        0.900\n",
      "            71          -0.19277        0.900\n",
      "            72          -0.19208        0.900\n",
      "            73          -0.19140        0.900\n",
      "            74          -0.19073        0.900\n",
      "            75          -0.19009        0.900\n",
      "            76          -0.18946        0.900\n",
      "            77          -0.18884        0.900\n",
      "            78          -0.18824        0.900\n",
      "            79          -0.18766        0.900\n",
      "            80          -0.18708        0.900\n",
      "            81          -0.18652        0.900\n",
      "            82          -0.18597        0.900\n",
      "            83          -0.18544        0.900\n",
      "            84          -0.18492        0.900\n",
      "            85          -0.18440        0.900\n",
      "            86          -0.18390        0.900\n",
      "            87          -0.18341        0.900\n",
      "            88          -0.18293        0.900\n",
      "            89          -0.18246        0.900\n",
      "            90          -0.18200        0.900\n",
      "            91          -0.18155        0.900\n",
      "            92          -0.18111        0.900\n",
      "            93          -0.18068        0.900\n",
      "            94          -0.18026        0.900\n",
      "            95          -0.17984        0.900\n",
      "            96          -0.17943        0.900\n",
      "            97          -0.17903        0.900\n",
      "            98          -0.17864        0.900\n",
      "            99          -0.17826        0.900\n",
      "         Final          -0.17788        0.900\n"
     ]
    }
   ],
   "source": [
    "## White\n",
    "white_tokens = makeTokens(['East European', 'West European', 'Jewish', 'Muslim'], train_df_w)\n",
    "white_classifier = mxec(white_tokens)\n",
    "white_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.333\n",
      "             2          -0.71315        0.818\n",
      "             3          -0.56806        0.837\n",
      "             4          -0.49074        0.852\n",
      "             5          -0.44072        0.865\n",
      "             6          -0.40470        0.875\n",
      "             7          -0.37699        0.883\n",
      "             8          -0.35472        0.891\n",
      "             9          -0.33625        0.896\n",
      "            10          -0.32060        0.901\n",
      "            11          -0.30709        0.905\n",
      "            12          -0.29527        0.909\n",
      "            13          -0.28482        0.911\n",
      "            14          -0.27549        0.913\n",
      "            15          -0.26710        0.915\n",
      "            16          -0.25950        0.917\n",
      "            17          -0.25259        0.918\n",
      "            18          -0.24626        0.919\n",
      "            19          -0.24044        0.920\n",
      "            20          -0.23508        0.921\n",
      "            21          -0.23011        0.922\n",
      "            22          -0.22550        0.922\n",
      "            23          -0.22121        0.923\n",
      "            24          -0.21720        0.923\n",
      "            25          -0.21345        0.924\n",
      "            26          -0.20993        0.924\n",
      "            27          -0.20662        0.924\n",
      "            28          -0.20350        0.925\n",
      "            29          -0.20056        0.925\n",
      "            30          -0.19778        0.925\n",
      "            31          -0.19515        0.925\n",
      "            32          -0.19266        0.925\n",
      "            33          -0.19029        0.926\n",
      "            34          -0.18804        0.926\n",
      "            35          -0.18589        0.926\n",
      "            36          -0.18385        0.926\n",
      "            37          -0.18191        0.926\n",
      "            38          -0.18004        0.926\n",
      "            39          -0.17827        0.926\n",
      "            40          -0.17656        0.926\n",
      "            41          -0.17493        0.926\n",
      "            42          -0.17337        0.926\n",
      "            43          -0.17187        0.926\n",
      "            44          -0.17043        0.926\n",
      "            45          -0.16904        0.926\n",
      "            46          -0.16771        0.926\n",
      "            47          -0.16643        0.926\n",
      "            48          -0.16519        0.927\n",
      "            49          -0.16400        0.927\n",
      "            50          -0.16285        0.927\n",
      "            51          -0.16174        0.927\n",
      "            52          -0.16067        0.927\n",
      "            53          -0.15964        0.927\n",
      "            54          -0.15863        0.927\n",
      "            55          -0.15767        0.927\n",
      "            56          -0.15673        0.927\n",
      "            57          -0.15582        0.927\n",
      "            58          -0.15494        0.927\n",
      "            59          -0.15409        0.927\n",
      "            60          -0.15326        0.927\n",
      "            61          -0.15246        0.927\n",
      "            62          -0.15168        0.927\n",
      "            63          -0.15092        0.927\n",
      "            64          -0.15019        0.927\n",
      "            65          -0.14947        0.927\n",
      "            66          -0.14878        0.927\n",
      "            67          -0.14810        0.927\n",
      "            68          -0.14744        0.927\n",
      "            69          -0.14680        0.927\n",
      "            70          -0.14618        0.927\n",
      "            71          -0.14557        0.927\n",
      "            72          -0.14498        0.927\n",
      "            73          -0.14440        0.927\n",
      "            74          -0.14384        0.927\n",
      "            75          -0.14329        0.927\n",
      "            76          -0.14275        0.927\n",
      "            77          -0.14223        0.927\n",
      "            78          -0.14172        0.927\n",
      "            79          -0.14122        0.927\n",
      "            80          -0.14074        0.927\n",
      "            81          -0.14026        0.927\n",
      "            82          -0.13980        0.927\n",
      "            83          -0.13934        0.927\n",
      "            84          -0.13890        0.927\n",
      "            85          -0.13846        0.927\n",
      "            86          -0.13804        0.927\n",
      "            87          -0.13762        0.927\n",
      "            88          -0.13721        0.927\n",
      "            89          -0.13681        0.927\n",
      "            90          -0.13642        0.927\n",
      "            91          -0.13604        0.927\n",
      "            92          -0.13567        0.927\n",
      "            93          -0.13530        0.927\n",
      "            94          -0.13494        0.927\n",
      "            95          -0.13459        0.927\n",
      "            96          -0.13424        0.927\n",
      "            97          -0.13390        0.927\n",
      "            98          -0.13357        0.927\n",
      "            99          -0.13324        0.927\n",
      "         Final          -0.13292        0.927\n"
     ]
    }
   ],
   "source": [
    "## Black\n",
    "black_tokens = makeTokens(['West European', 'Continental African', 'Muslim'], train_df_w)\n",
    "black_classifier = mxec(black_tokens)\n",
    "black_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.333\n",
      "             2          -0.62121        0.862\n",
      "             3          -0.46739        0.880\n",
      "             4          -0.39155        0.895\n",
      "             5          -0.34462        0.904\n",
      "             6          -0.31182        0.914\n",
      "             7          -0.28712        0.922\n",
      "             8          -0.26758        0.927\n",
      "             9          -0.25159        0.932\n",
      "            10          -0.23817        0.937\n",
      "            11          -0.22669        0.940\n",
      "            12          -0.21672        0.942\n",
      "            13          -0.20795        0.944\n",
      "            14          -0.20017        0.946\n",
      "            15          -0.19320        0.947\n",
      "            16          -0.18692        0.948\n",
      "            17          -0.18122        0.949\n",
      "            18          -0.17603        0.950\n",
      "            19          -0.17128        0.951\n",
      "            20          -0.16690        0.951\n",
      "            21          -0.16287        0.952\n",
      "            22          -0.15913        0.952\n",
      "            23          -0.15565        0.953\n",
      "            24          -0.15241        0.953\n",
      "            25          -0.14939        0.954\n",
      "            26          -0.14656        0.954\n",
      "            27          -0.14390        0.954\n",
      "            28          -0.14140        0.954\n",
      "            29          -0.13905        0.954\n",
      "            30          -0.13683        0.955\n",
      "            31          -0.13473        0.955\n",
      "            32          -0.13274        0.955\n",
      "            33          -0.13085        0.955\n",
      "            34          -0.12906        0.955\n",
      "            35          -0.12736        0.955\n",
      "            36          -0.12574        0.955\n",
      "            37          -0.12419        0.955\n",
      "            38          -0.12272        0.955\n",
      "            39          -0.12131        0.955\n",
      "            40          -0.11996        0.955\n",
      "            41          -0.11867        0.955\n",
      "            42          -0.11743        0.955\n",
      "            43          -0.11625        0.955\n",
      "            44          -0.11511        0.955\n",
      "            45          -0.11402        0.955\n",
      "            46          -0.11297        0.955\n",
      "            47          -0.11196        0.955\n",
      "            48          -0.11098        0.955\n",
      "            49          -0.11004        0.955\n",
      "            50          -0.10914        0.955\n",
      "            51          -0.10827        0.955\n",
      "            52          -0.10742        0.955\n",
      "            53          -0.10661        0.955\n",
      "            54          -0.10582        0.955\n",
      "            55          -0.10506        0.955\n",
      "            56          -0.10433        0.955\n",
      "            57          -0.10361        0.955\n",
      "            58          -0.10292        0.955\n",
      "            59          -0.10225        0.955\n",
      "            60          -0.10160        0.955\n",
      "            61          -0.10097        0.956\n",
      "            62          -0.10036        0.956\n",
      "            63          -0.09977        0.956\n",
      "            64          -0.09919        0.956\n",
      "            65          -0.09863        0.956\n",
      "            66          -0.09809        0.956\n",
      "            67          -0.09756        0.956\n",
      "            68          -0.09704        0.956\n",
      "            69          -0.09654        0.956\n",
      "            70          -0.09605        0.956\n",
      "            71          -0.09558        0.956\n",
      "            72          -0.09511        0.956\n",
      "            73          -0.09466        0.956\n",
      "            74          -0.09422        0.956\n",
      "            75          -0.09379        0.956\n",
      "            76          -0.09337        0.956\n",
      "            77          -0.09296        0.956\n",
      "            78          -0.09257        0.956\n",
      "            79          -0.09218        0.956\n",
      "            80          -0.09180        0.956\n",
      "            81          -0.09142        0.956\n",
      "            82          -0.09106        0.956\n",
      "            83          -0.09070        0.956\n",
      "            84          -0.09036        0.956\n",
      "            85          -0.09002        0.956\n",
      "            86          -0.08969        0.956\n",
      "            87          -0.08936        0.956\n",
      "            88          -0.08904        0.956\n",
      "            89          -0.08873        0.956\n",
      "            90          -0.08842        0.956\n",
      "            91          -0.08813        0.956\n",
      "            92          -0.08783        0.956\n",
      "            93          -0.08755        0.956\n",
      "            94          -0.08727        0.956\n",
      "            95          -0.08699        0.956\n",
      "            96          -0.08672        0.956\n",
      "            97          -0.08646        0.956\n",
      "            98          -0.08620        0.956\n",
      "            99          -0.08594        0.956\n",
      "         Final          -0.08569        0.956\n"
     ]
    }
   ],
   "source": [
    "## Asian\n",
    "asian_tokens = makeTokens(['East Asian', 'Indian', 'Muslim'], train_df_w)\n",
    "asian_classifier = mxec(asian_tokens)\n",
    "asian_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hispanic will be their own ethnic group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Native American will be their own ethnic group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Revised Model\n",
    "\n",
    "We will test our model on the baby names test set to stay consistent with our testing criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample size: 7994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18601</th>\n",
       "      <td>Mangat</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11774</th>\n",
       "      <td>Natal</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15934</th>\n",
       "      <td>Porath</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>Quarnstrom</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>Andersson</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17885</th>\n",
       "      <td>Rais</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>Reichenheim</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4102</th>\n",
       "      <td>Levert</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13202</th>\n",
       "      <td>Grimme</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>Machart</td>\n",
       "      <td>East European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12658</th>\n",
       "      <td>Chidi</td>\n",
       "      <td>Continental African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15830</th>\n",
       "      <td>Orenthal</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14941</th>\n",
       "      <td>Grossinger</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15612</th>\n",
       "      <td>Maza</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>Oliveira</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name       True Ethnicity\n",
       "18601       Mangat               Indian\n",
       "11774        Natal             Hispanic\n",
       "15934       Porath               Jewish\n",
       "9899    Quarnstrom        West European\n",
       "9106     Andersson        West European\n",
       "17885         Rais        West European\n",
       "15998  Reichenheim               Jewish\n",
       "4102        Levert        West European\n",
       "13202       Grimme        West European\n",
       "7935       Machart        East European\n",
       "12658        Chidi  Continental African\n",
       "15830     Orenthal               Jewish\n",
       "14941   Grossinger               Jewish\n",
       "15612         Maza               Jewish\n",
       "19987     Oliveira             Hispanic"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Consolidation function: Combines similar ethnicities together into groups\n",
    "# Example: 'russian','ukranian','czech','slavic' become \"East European\"\n",
    "\n",
    "super_list_names = []\n",
    "super_list_ethnicities = []\n",
    "\n",
    "for ethnicity in ethnicities:\n",
    "    name_list = eth_dict[ethnicity][0]\n",
    "    eth_list = []\n",
    "    for name in name_list:\n",
    "        eth_list.append(ethnicity)\n",
    "    super_list_names = super_list_names + name_list\n",
    "    super_list_ethnicities = super_list_ethnicities + eth_list\n",
    "    \n",
    "df = pd.DataFrame(\n",
    "            {'Name': super_list_names,\n",
    "             'True Ethnicity': super_list_ethnicities\n",
    "            })\n",
    "\n",
    "df_c = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "\n",
    "def consolidate(eth_list, target_df, consolidated_eth):\n",
    "    sample_df = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "    for ethnicity in eth_list:\n",
    "        eth_df = df[df['True Ethnicity']==ethnicity]\n",
    "        n_per_eth = (1000 / len(eth_list))\n",
    "        sample_df = pd.concat([sample_df, eth_df.sample(n=n_per_eth, replace = True)])\n",
    "    sample_df['True Ethnicity'] = consolidated_eth\n",
    "    return pd.concat([target_df,sample_df]) \n",
    "\n",
    "\n",
    "# Consolidate East European\n",
    "east_euro = ['russian','ukranian','czech','slavic']\n",
    "df_c = consolidate(east_euro, df_c, 'East European')\n",
    "\n",
    "# Consolidate West European\n",
    "west_euro = ['italian','irish','danish','french',\n",
    "                'swedish','german','swiss']\n",
    "df_c = consolidate(west_euro, df_c, 'West European')\n",
    "\n",
    "# Consolidate Muslim / Arab\n",
    "muslim_arabic = ['muslim', 'arabic']\n",
    "df_c = consolidate(muslim_arabic, df_c, 'Muslim')\n",
    "\n",
    "# Consolidate East Asian\n",
    "east_asian = ['chinese','japanese','vietnamese','korean']\n",
    "df_c = consolidate(east_asian, df_c, 'East Asian')\n",
    "\n",
    "# Spanish / Hispanic can remain its own category\n",
    "hispanic = ['spanish','portugese'] \n",
    "df_c = consolidate(hispanic, df_c, 'Hispanic')\n",
    "\n",
    "# Jewish can remain its own category\n",
    "jewish = ['jewish']\n",
    "df_c = consolidate(jewish, df_c, 'Jewish')\n",
    "\n",
    "# Indian can remain its own category\n",
    "indian = ['indian']\n",
    "df_c = consolidate(indian, df_c, 'Indian')\n",
    "\n",
    "# African can remain its own category \n",
    "african = ['african']\n",
    "df_c = consolidate(african, df_c, 'Continental African')\n",
    "\n",
    "print 'Cleaned sample size:', len(df_c)\n",
    "df_c.sample(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split test set into subsets by race\n",
    "test_df_black = df_c[df_c['True Ethnicity'].isin(['West European', 'Continental African', 'Muslim'])]\n",
    "test_df_asian = df_c[df_c['True Ethnicity'].isin(['East Asian', 'Indian', 'Muslim'])]\n",
    "test_df_white = df_c[df_c['True Ethnicity'].isin(['East European', 'West European', 'Jewish', 'Muslim'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shaer</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sharaf</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kazmi</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niazi</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kazemi</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dajani</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saeed</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Othman</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Doud</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Harron</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mohamed</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mady</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Saade</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fares</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Azzam</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Shabazz</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Shaker</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tahir</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hosein</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Saah</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Assaf</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Nazar</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bagheri</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Massoud</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Badour</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Salim</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Beydoun</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Elbaz</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Faraj</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>Kannan</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>Din</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>Samra</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>Baria</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>Chahal</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>Venkatesh</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>Bhardwaj</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>Manne</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>Ranganathan</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>Sane</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>Sekhon</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>Nayak</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>Rastogi</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>Toor</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>Goda</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>Tiwari</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>Kapur</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>Bal</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>Rastogi</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>Chaudhary</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>Gara</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>Chaudhari</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>Sarma</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>Palla</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>Mohanty</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>Bains</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Goda</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Grover</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Biswas</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Jani</td>\n",
       "      <td>Muslim</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Prediction True Ethnicity\n",
       "0           Shaer      Muslim         Muslim\n",
       "1          Sharaf      Muslim         Muslim\n",
       "2             Jan      Muslim         Muslim\n",
       "3           Kazmi      Muslim         Muslim\n",
       "4           Niazi      Indian         Muslim\n",
       "5          Kazemi  East Asian         Muslim\n",
       "6          Dajani      Muslim         Muslim\n",
       "7           Saeed      Muslim         Muslim\n",
       "8          Othman      Muslim         Muslim\n",
       "9            Doud      Muslim         Muslim\n",
       "10         Harron      Indian         Muslim\n",
       "11        Mohamed      Muslim         Muslim\n",
       "12           Mady      Indian         Muslim\n",
       "13          Saade      Muslim         Muslim\n",
       "14          Fares      Muslim         Muslim\n",
       "15          Azzam      Muslim         Muslim\n",
       "16        Shabazz      Muslim         Muslim\n",
       "17         Shaker      Muslim         Muslim\n",
       "18          Tahir      Muslim         Muslim\n",
       "19         Hosein  East Asian         Muslim\n",
       "20           Saah      Muslim         Muslim\n",
       "21          Assaf      Muslim         Muslim\n",
       "22          Nazar      Muslim         Muslim\n",
       "23        Bagheri      Indian         Muslim\n",
       "24        Massoud      Muslim         Muslim\n",
       "25         Badour      Muslim         Muslim\n",
       "26          Salim      Muslim         Muslim\n",
       "27        Beydoun      Muslim         Muslim\n",
       "28          Elbaz      Muslim         Muslim\n",
       "29          Faraj      Muslim         Muslim\n",
       "...           ...         ...            ...\n",
       "2970       Kannan      Indian         Indian\n",
       "2971          Din      Indian         Indian\n",
       "2972        Samra      Muslim         Indian\n",
       "2973        Baria      Indian         Indian\n",
       "2974       Chahal      Indian         Indian\n",
       "2975    Venkatesh      Indian         Indian\n",
       "2976     Bhardwaj      Indian         Indian\n",
       "2977        Manne      Indian         Indian\n",
       "2978  Ranganathan      Indian         Indian\n",
       "2979         Sane  East Asian         Indian\n",
       "2980       Sekhon  East Asian         Indian\n",
       "2981        Nayak      Indian         Indian\n",
       "2982      Rastogi      Indian         Indian\n",
       "2983         Toor      Indian         Indian\n",
       "2984         Goda      Indian         Indian\n",
       "2985       Tiwari      Indian         Indian\n",
       "2986        Kapur      Indian         Indian\n",
       "2987          Bal      Indian         Indian\n",
       "2988      Rastogi      Indian         Indian\n",
       "2989    Chaudhary      Indian         Indian\n",
       "2990         Gara      Muslim         Indian\n",
       "2991    Chaudhari      Indian         Indian\n",
       "2992        Sarma      Indian         Indian\n",
       "2993        Palla      Indian         Indian\n",
       "2994      Mohanty      Indian         Indian\n",
       "2995        Bains      Indian         Indian\n",
       "2996         Goda      Indian         Indian\n",
       "2997       Grover      Indian         Indian\n",
       "2998       Biswas      Indian         Indian\n",
       "2999         Jani      Muslim         Indian\n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on racially split test sets!!\n",
    "\n",
    "def makePreds(test_df, classifier): \n",
    "    test_names = list(test_df['Name'])\n",
    "    test_eth = list(test_df['True Ethnicity'])\n",
    "\n",
    "    test_preds = []\n",
    "\n",
    "    for name in test_names:\n",
    "        pred = classifier.classify(name)\n",
    "        test_preds.append(pred)\n",
    "\n",
    "    df_preds = pd.DataFrame({\n",
    "        'Name': test_names,\n",
    "        'True Ethnicity': test_eth,\n",
    "        'Prediction': test_preds\n",
    "    })\n",
    "    return df_preds\n",
    "\n",
    "black_p = makePreds(test_df_black, black_classifier)\n",
    "asian_p = makePreds(test_df_asian, asian_classifier)\n",
    "white_p = makePreds(test_df_white, white_classifier)\n",
    "\n",
    "asian_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Accuracy Table Generator\n",
    "\n",
    "\n",
    "def accuracyTable(df_preds):\n",
    "    accuracies = []\n",
    "    TPs = [] # number of times predict X when  X ethnicity\n",
    "    FPs = [] # number of times predict X when 'X ethnicity\n",
    "    TNs = [] # number of times predict'X when 'X ethnicity\n",
    "    FNs = [] # number of times predict'X when  X ethnicity\n",
    "\n",
    "\n",
    "    ethnicities_c = list(df_preds['True Ethnicity'].unique())\n",
    "    ethnicity_list = []\n",
    "    \n",
    "    # Classification Accuracy\n",
    "    for ethnicity in ethnicities_c:\n",
    "        #accuracy = calcAccuracy(df_preds[df_preds['True Ethnicity']==ethnicity])\n",
    "        #accuracies.append(accuracy)\n",
    "        TPs.append(calcTP(df_preds, ethnicity))\n",
    "        FPs.append(calcFP(df_preds, ethnicity))\n",
    "        TNs.append(calcTN(df_preds, ethnicity))\n",
    "        FNs.append(calcFN(df_preds, ethnicity))\n",
    "        ethnicity_list.append(ethnicity)\n",
    "\n",
    "    # Aggregate accuracy\n",
    "    #accuracies.append(calcAccuracy(df_preds))\n",
    "    #ethnicity_list.append('OVERALL')\n",
    "\n",
    "    # put into df\n",
    "    df_acc = pd.DataFrame({\n",
    "        'True Ethnicity': ethnicity_list,\n",
    "        #'Classification Accuracy': accuracies,\n",
    "        'TP': TPs,\n",
    "        'FP': FPs,\n",
    "        'TN': TNs,\n",
    "        'FN': FNs\n",
    "    })\n",
    "\n",
    "    df_acc.set_index('True Ethnicity', inplace=True)\n",
    "\n",
    "    # Add TPR (Sensistivity)\n",
    "    df_acc['Sensitivity (TPR)'] = (df_acc['TP']) / (df_acc['TP'] + df_acc['FN'])\n",
    "\n",
    "    # Add FPR\n",
    "    df_acc['FPR'] = (df_acc['FP']) / (df_acc['FP'] + df_acc['TN'])\n",
    "\n",
    "    # Add Precision\n",
    "    df_acc['Precision'] = (df_acc['TP']) / (df_acc['TP'] + df_acc['FP'])\n",
    "\n",
    "    # F1 Score (harmonic mean of precision and sensitivity)\n",
    "    df_acc['F1 Score'] = (2 * df_acc['TP'] / ((2*df_acc['TP'])+df_acc['FP']+df_acc['FN']))\n",
    "\n",
    "    # Accuracy (ACC)\n",
    "    df_acc['ACC'] = (df_acc['TP']+df_acc['TN']) / (df_acc['TP']+df_acc['TN']+df_acc['FP']+df_acc['FN'])\n",
    "    \n",
    "    return df_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asian_table = accuracyTable(asian_p)\n",
    "black_table = accuracyTable(black_p)\n",
    "white_table = accuracyTable(white_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity (TPR)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Muslim</th>\n",
       "      <td>192</td>\n",
       "      <td>180</td>\n",
       "      <td>1820</td>\n",
       "      <td>808</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.817814</td>\n",
       "      <td>0.812877</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>East Asian</th>\n",
       "      <td>187</td>\n",
       "      <td>130</td>\n",
       "      <td>1870</td>\n",
       "      <td>813</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.862142</td>\n",
       "      <td>0.836850</td>\n",
       "      <td>0.894333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>172</td>\n",
       "      <td>241</td>\n",
       "      <td>1759</td>\n",
       "      <td>828</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>0.774556</td>\n",
       "      <td>0.800387</td>\n",
       "      <td>0.862333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 FN   FP    TN   TP  Sensitivity (TPR)     FPR  Precision  \\\n",
       "True Ethnicity                                                              \n",
       "Muslim          192  180  1820  808              0.808  0.0900   0.817814   \n",
       "East Asian      187  130  1870  813              0.813  0.0650   0.862142   \n",
       "Indian          172  241  1759  828              0.828  0.1205   0.774556   \n",
       "\n",
       "                F1 Score       ACC  \n",
       "True Ethnicity                      \n",
       "Muslim          0.812877  0.876000  \n",
       "East Asian      0.836850  0.894333  \n",
       "Indian          0.800387  0.862333  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asian_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity (TPR)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>West European</th>\n",
       "      <td>275</td>\n",
       "      <td>133</td>\n",
       "      <td>1867</td>\n",
       "      <td>719</td>\n",
       "      <td>0.72334</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.843897</td>\n",
       "      <td>0.778982</td>\n",
       "      <td>0.863727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Muslim</th>\n",
       "      <td>311</td>\n",
       "      <td>153</td>\n",
       "      <td>1841</td>\n",
       "      <td>689</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.076730</td>\n",
       "      <td>0.818290</td>\n",
       "      <td>0.748100</td>\n",
       "      <td>0.845023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continental African</th>\n",
       "      <td>110</td>\n",
       "      <td>410</td>\n",
       "      <td>1584</td>\n",
       "      <td>890</td>\n",
       "      <td>0.89000</td>\n",
       "      <td>0.205617</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.826319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      FN   FP    TN   TP  Sensitivity (TPR)       FPR  \\\n",
       "True Ethnicity                                                          \n",
       "West European        275  133  1867  719            0.72334  0.066500   \n",
       "Muslim               311  153  1841  689            0.68900  0.076730   \n",
       "Continental African  110  410  1584  890            0.89000  0.205617   \n",
       "\n",
       "                     Precision  F1 Score       ACC  \n",
       "True Ethnicity                                      \n",
       "West European         0.843897  0.778982  0.863727  \n",
       "Muslim                0.818290  0.748100  0.845023  \n",
       "Continental African   0.684615  0.773913  0.826319  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity (TPR)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>East European</th>\n",
       "      <td>491</td>\n",
       "      <td>266</td>\n",
       "      <td>2728</td>\n",
       "      <td>509</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.088844</td>\n",
       "      <td>0.656774</td>\n",
       "      <td>0.573521</td>\n",
       "      <td>0.810466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West European</th>\n",
       "      <td>430</td>\n",
       "      <td>334</td>\n",
       "      <td>2666</td>\n",
       "      <td>564</td>\n",
       "      <td>0.567404</td>\n",
       "      <td>0.111333</td>\n",
       "      <td>0.628062</td>\n",
       "      <td>0.596195</td>\n",
       "      <td>0.808713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Muslim</th>\n",
       "      <td>222</td>\n",
       "      <td>287</td>\n",
       "      <td>2707</td>\n",
       "      <td>778</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.095858</td>\n",
       "      <td>0.730516</td>\n",
       "      <td>0.753511</td>\n",
       "      <td>0.872559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewish</th>\n",
       "      <td>404</td>\n",
       "      <td>660</td>\n",
       "      <td>2334</td>\n",
       "      <td>596</td>\n",
       "      <td>0.596000</td>\n",
       "      <td>0.220441</td>\n",
       "      <td>0.474522</td>\n",
       "      <td>0.528369</td>\n",
       "      <td>0.733600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 FN   FP    TN   TP  Sensitivity (TPR)       FPR  Precision  \\\n",
       "True Ethnicity                                                                \n",
       "East European   491  266  2728  509           0.509000  0.088844   0.656774   \n",
       "West European   430  334  2666  564           0.567404  0.111333   0.628062   \n",
       "Muslim          222  287  2707  778           0.778000  0.095858   0.730516   \n",
       "Jewish          404  660  2334  596           0.596000  0.220441   0.474522   \n",
       "\n",
       "                F1 Score       ACC  \n",
       "True Ethnicity                      \n",
       "East European   0.573521  0.810466  \n",
       "West European   0.596195  0.808713  \n",
       "Muslim          0.753511  0.872559  \n",
       "Jewish          0.528369  0.733600  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply predictions to Voter Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import data previously cleaned by Riddhi and Kimia\n",
    "df_voters = pd.read_csv('with race.csv')\n",
    "len(df_voters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that pulls the probability upon which our classifier is making its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0957566641707\n",
      "-0.426646745245\n",
      "-0.145693472201\n"
     ]
    }
   ],
   "source": [
    "## Generate probabilities\n",
    "\n",
    "import operator\n",
    "def black_proba(name):\n",
    "    probs = black_classifier.prob_classify(str(name))._prob_dict.items()\n",
    "    probs.sort(key=operator.itemgetter(1))\n",
    "    return probs[-1][1]\n",
    "\n",
    "def asian_proba(name):\n",
    "    probs = asian_classifier.prob_classify(str(name))._prob_dict.items()\n",
    "    probs.sort(key=operator.itemgetter(1))\n",
    "    return probs[-1][1]\n",
    "\n",
    "def white_proba(name):\n",
    "    probs = white_classifier.prob_classify(str(name))._prob_dict.items()\n",
    "    probs.sort(key=operator.itemgetter(1))\n",
    "    return probs[-1][1]\n",
    "\n",
    "# finds negative log likelihood of prediction as log (lower is better)\n",
    "print black_proba('Halal')\n",
    "print white_proba('Halal')\n",
    "print asian_proba('Halal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ These are actual negative log likelihood values, which are evaluating the ability of the model to fit this name. Higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get back to parsing our voter file for important things like race and name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull important columns out of big dataframe\n",
    "name = df_voters['LAST_NAME']\n",
    "race_prior = df_voters['19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's turn Race into a string instead of INT, for comprehensibility\n",
    "race_prior2 = []\n",
    "for row in range(len(race_prior)):\n",
    "    if race_prior[row] == 5:   #white\n",
    "        race_prior2.append('White')\n",
    "    elif race_prior[row] == 2: #asian\n",
    "        race_prior2.append('Asian')\n",
    "    elif race_prior[row] == 3: #black \n",
    "        race_prior2.append('Black')\n",
    "    elif race_prior[row] == 1: #American Indian\n",
    "        race_prior2.append('American Indian')\n",
    "    elif race_prior[row] == 4: #Hispanic\n",
    "        race_prior2.append('Hispanic')\n",
    "    else:                      #Others\n",
    "        race_prior2.append('Other / Mixed')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the predictions into our Voter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ethnicity_predictions = []\n",
    "pred_score = []\n",
    "abstain_predictions = []\n",
    "\n",
    "\n",
    "## Make the predictions from the ensemble model\n",
    "\n",
    "for row in range(len(df_voters)):\n",
    "    if race_prior[row] == 5:   #white\n",
    "        eth = white_classifier.classify(str(name[row]))\n",
    "        prob = white_proba(name[row])\n",
    "        ethnicity_predictions.append(eth)\n",
    "        pred_score.append(prob)\n",
    "        if prob > -0.75: # threshold for abstaining\n",
    "            abstain_predictions.append(eth)\n",
    "        else:\n",
    "            abstain_predictions.append('Abstain')\n",
    "    elif race_prior[row] == 2: #asian\n",
    "        eth = asian_classifier.classify(str(name[row]))\n",
    "        prob = asian_proba(name[row])\n",
    "        ethnicity_predictions.append(eth)\n",
    "        pred_score.append(prob)\n",
    "        if prob > -0.75:\n",
    "            abstain_predictions.append(eth)\n",
    "        else:\n",
    "            abstain_predictions.append('Abstain')\n",
    "    elif race_prior[row] == 3: #black \n",
    "        eth = black_classifier.classify(str(name[row]))\n",
    "        prob = black_proba(name[row])\n",
    "        ethnicity_predictions.append(eth)\n",
    "        pred_score.append(prob)\n",
    "        if prob > -0.75:\n",
    "            abstain_predictions.append(eth)\n",
    "        else:\n",
    "            abstain_predictions.append('Abstain')\n",
    "    elif race_prior[row] == 1: #American Indian\n",
    "        eth = 'American Indian'\n",
    "        prob = 0.0\n",
    "        ethnicity_predictions.append(eth)\n",
    "        pred_score.append(prob)\n",
    "        if prob > -0.75:\n",
    "            abstain_predictions.append(eth)\n",
    "        else:\n",
    "            abstain_predictions.append('Abstain')\n",
    "    elif race_prior[row] == 4: #Hispanic\n",
    "        eth = 'Hispanic'\n",
    "        prob = 0.0\n",
    "        ethnicity_predictions.append(eth)\n",
    "        pred_score.append(prob)\n",
    "        if prob > -0.75:\n",
    "            abstain_predictions.append(eth)\n",
    "        else:\n",
    "            abstain_predictions.append('Abstain')\n",
    "    else:                      #Others\n",
    "        eth = 'Abstain'\n",
    "        prob = -10.0\n",
    "        ethnicity_predictions.append(eth)\n",
    "        pred_score.append(prob)\n",
    "        abstain_predictions.append('Abstain')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's relabel some of our ethnicities to make their labels more informative.\n",
    "\n",
    "* e.g. A Black person with a \"West European\" name classification becomes \"Blackamerican\" instead of \"Western European\".\n",
    "* e.g. A Black person with a \"Muslim\" name classification becomes \"Black Muslim\" instead of simply Muslim.\n",
    "\n",
    "There may be trends we can see with these more grainular ethnicity distinctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relabel ethnicities\n",
    "def label(race, eth):\n",
    "    new_eths = []\n",
    "    if len(eth) ==  len(race):\n",
    "        print 'List lengths are equivalent, good.'\n",
    "    for row in range(len(eth)):\n",
    "        if race[row] == 'White':\n",
    "            if eth[row] == 'Muslim':\n",
    "                new_eths.append('Arab Muslim')\n",
    "            else:\n",
    "                new_eths.append(eth[row])\n",
    "        elif race[row] == 'Black':\n",
    "            if eth[row] == 'Muslim':\n",
    "                new_eths.append('Black Muslim')\n",
    "            elif eth[row] == 'West European':\n",
    "                new_eths.append('Blackamerican')\n",
    "            else:\n",
    "                new_eths.append(eth[row])        \n",
    "        elif race[row] == 'Asian':\n",
    "            if eth[row] == 'Muslim':\n",
    "                new_eths.append('Asian Muslim')\n",
    "            elif eth[row] == 'Indian':\n",
    "                new_eths.append('Indian Subcont.')\n",
    "            else:\n",
    "                new_eths.append(eth[row])            \n",
    "        else:\n",
    "            new_eths.append(eth[row])\n",
    "    return new_eths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List lengths are equivalent, good.\n",
      "List lengths are equivalent, good.\n"
     ]
    }
   ],
   "source": [
    "df_voters['Ethnicity Prediction'] = label(race_prior2, ethnicity_predictions)\n",
    "df_voters['Prediction Score'] = pred_score\n",
    "df_voters['Conservative Ethnicity Prediction'] = label(race_prior2,abstain_predictions)\n",
    "df_voters['Race (Prior)'] = race_prior2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our predictions and the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>Race (Prior)</th>\n",
       "      <th>Ethnicity Prediction</th>\n",
       "      <th>Prediction Score</th>\n",
       "      <th>Conservative Ethnicity Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85982</th>\n",
       "      <td>MATYLANGE</td>\n",
       "      <td>Black</td>\n",
       "      <td>Continental African</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54106</th>\n",
       "      <td>MAHONIK</td>\n",
       "      <td>White</td>\n",
       "      <td>East European</td>\n",
       "      <td>-1.605525</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25209</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-0.838545</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27516</th>\n",
       "      <td>Porter</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-0.607304</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86434</th>\n",
       "      <td>Rodriguez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>Santana</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11562</th>\n",
       "      <td>Monk</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.604397</td>\n",
       "      <td>Blackamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83412</th>\n",
       "      <td>Ceaser</td>\n",
       "      <td>Other / Mixed</td>\n",
       "      <td>Abstain</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77012</th>\n",
       "      <td>Snider</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.566677</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81044</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-0.700196</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87601</th>\n",
       "      <td>JENKINS</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-0.478807</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36485</th>\n",
       "      <td>Risi</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.274100</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67015</th>\n",
       "      <td>Gonzalez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47065</th>\n",
       "      <td>Sainsbury</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.295834</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15569</th>\n",
       "      <td>Bedoya</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83957</th>\n",
       "      <td>Westbrook</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.372576</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33917</th>\n",
       "      <td>Norton</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.277361</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57847</th>\n",
       "      <td>Bingham</td>\n",
       "      <td>White</td>\n",
       "      <td>Arab Muslim</td>\n",
       "      <td>-0.939497</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82310</th>\n",
       "      <td>Mueller</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.027081</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6941</th>\n",
       "      <td>Gervais</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.391841</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67531</th>\n",
       "      <td>BRYON</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.529831</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>HINDS</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black Muslim</td>\n",
       "      <td>-1.057007</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69095</th>\n",
       "      <td>THOM</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.960518</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65119</th>\n",
       "      <td>Luckett</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.973180</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79989</th>\n",
       "      <td>Case</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.490454</td>\n",
       "      <td>Blackamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23261</th>\n",
       "      <td>GUERRERO</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93019</th>\n",
       "      <td>Lacy</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.685518</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15214</th>\n",
       "      <td>SNYDER</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-0.597038</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38586</th>\n",
       "      <td>Lucas-Dorsey</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.211128</td>\n",
       "      <td>Blackamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27441</th>\n",
       "      <td>Goethel</td>\n",
       "      <td>American Indian</td>\n",
       "      <td>American Indian</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>American Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47703</th>\n",
       "      <td>PRICE</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.117419</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66827</th>\n",
       "      <td>BIAMONTE</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.045815</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91207</th>\n",
       "      <td>Lopez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36392</th>\n",
       "      <td>RICOTTA</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-0.664569</td>\n",
       "      <td>West European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>SHARP</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.317547</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378</th>\n",
       "      <td>RYMER</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.448828</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25314</th>\n",
       "      <td>WALKER</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black Muslim</td>\n",
       "      <td>-0.912935</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64848</th>\n",
       "      <td>MURSCH</td>\n",
       "      <td>White</td>\n",
       "      <td>East European</td>\n",
       "      <td>-1.634483</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21850</th>\n",
       "      <td>Lamason</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.239834</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49218</th>\n",
       "      <td>Jarusiewicz</td>\n",
       "      <td>White</td>\n",
       "      <td>East European</td>\n",
       "      <td>-0.510897</td>\n",
       "      <td>East European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31921</th>\n",
       "      <td>Hicks</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black Muslim</td>\n",
       "      <td>-0.783272</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60812</th>\n",
       "      <td>Bryant</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-0.791560</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15478</th>\n",
       "      <td>Patel</td>\n",
       "      <td>Other / Mixed</td>\n",
       "      <td>Abstain</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82349</th>\n",
       "      <td>CHRIST</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.167648</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70790</th>\n",
       "      <td>Best</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.540298</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60803</th>\n",
       "      <td>HENCH</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.428192</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92023</th>\n",
       "      <td>CROWLEY</td>\n",
       "      <td>Other / Mixed</td>\n",
       "      <td>Abstain</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43832</th>\n",
       "      <td>Valmocina</td>\n",
       "      <td>White</td>\n",
       "      <td>East European</td>\n",
       "      <td>-1.263331</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55615</th>\n",
       "      <td>Perez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86878</th>\n",
       "      <td>OBRIEN</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.445590</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99286</th>\n",
       "      <td>Parker</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.253762</td>\n",
       "      <td>Blackamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72053</th>\n",
       "      <td>Lang</td>\n",
       "      <td>White</td>\n",
       "      <td>West European</td>\n",
       "      <td>-1.129779</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82666</th>\n",
       "      <td>Daniels</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.123954</td>\n",
       "      <td>Blackamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7630</th>\n",
       "      <td>GAINES</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blackamerican</td>\n",
       "      <td>-0.560999</td>\n",
       "      <td>Blackamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>Rodriguez-Arellano</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12857</th>\n",
       "      <td>Perez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90992</th>\n",
       "      <td>Fuentes</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69098</th>\n",
       "      <td>Diaz Rivera</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47722</th>\n",
       "      <td>Gilmer</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-1.218038</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>SCHERER</td>\n",
       "      <td>White</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>-0.910811</td>\n",
       "      <td>Abstain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                LAST_NAME     Race (Prior) Ethnicity Prediction  \\\n",
       "85982           MATYLANGE            Black  Continental African   \n",
       "54106             MAHONIK            White        East European   \n",
       "25209             Carroll            White        West European   \n",
       "27516              Porter            White               Jewish   \n",
       "86434           Rodriguez         Hispanic             Hispanic   \n",
       "4487              Santana         Hispanic             Hispanic   \n",
       "11562                Monk            Black        Blackamerican   \n",
       "83412              Ceaser    Other / Mixed              Abstain   \n",
       "77012              Snider            White               Jewish   \n",
       "81044             Stewart            White        West European   \n",
       "87601             JENKINS            White               Jewish   \n",
       "36485                Risi            White        West European   \n",
       "67015            Gonzalez         Hispanic             Hispanic   \n",
       "47065           Sainsbury            White               Jewish   \n",
       "15569              Bedoya         Hispanic             Hispanic   \n",
       "83957           Westbrook            White               Jewish   \n",
       "33917              Norton            White               Jewish   \n",
       "57847             Bingham            White          Arab Muslim   \n",
       "82310             Mueller            White               Jewish   \n",
       "6941              Gervais            White        West European   \n",
       "67531               BRYON            White               Jewish   \n",
       "1999                HINDS            Black         Black Muslim   \n",
       "69095                THOM            Black        Blackamerican   \n",
       "65119             Luckett            Black        Blackamerican   \n",
       "79989                Case            Black        Blackamerican   \n",
       "23261            GUERRERO         Hispanic             Hispanic   \n",
       "93019                Lacy            White        West European   \n",
       "15214              SNYDER            White               Jewish   \n",
       "38586        Lucas-Dorsey            Black        Blackamerican   \n",
       "27441             Goethel  American Indian      American Indian   \n",
       "...                   ...              ...                  ...   \n",
       "47703               PRICE            White               Jewish   \n",
       "66827            BIAMONTE            White        West European   \n",
       "91207               Lopez         Hispanic             Hispanic   \n",
       "36392             RICOTTA            White        West European   \n",
       "3019                SHARP            White               Jewish   \n",
       "10378               RYMER            White               Jewish   \n",
       "25314              WALKER            Black         Black Muslim   \n",
       "64848              MURSCH            White        East European   \n",
       "21850             Lamason            White               Jewish   \n",
       "49218         Jarusiewicz            White        East European   \n",
       "31921               Hicks            Black         Black Muslim   \n",
       "60812              Bryant            White        West European   \n",
       "15478               Patel    Other / Mixed              Abstain   \n",
       "82349              CHRIST            White        West European   \n",
       "70790                Best            White               Jewish   \n",
       "60803               HENCH            White        West European   \n",
       "92023             CROWLEY    Other / Mixed              Abstain   \n",
       "43832           Valmocina            White        East European   \n",
       "55615               Perez         Hispanic             Hispanic   \n",
       "86878              OBRIEN            White               Jewish   \n",
       "99286              Parker            Black        Blackamerican   \n",
       "72053                Lang            White        West European   \n",
       "82666             Daniels            Black        Blackamerican   \n",
       "7630               GAINES            Black        Blackamerican   \n",
       "1577   Rodriguez-Arellano         Hispanic             Hispanic   \n",
       "12857               Perez         Hispanic             Hispanic   \n",
       "90992             Fuentes         Hispanic             Hispanic   \n",
       "69098         Diaz Rivera         Hispanic             Hispanic   \n",
       "47722              Gilmer            White               Jewish   \n",
       "7718              SCHERER            White               Jewish   \n",
       "\n",
       "       Prediction Score Conservative Ethnicity Prediction  \n",
       "85982         -1.055064                           Abstain  \n",
       "54106         -1.605525                           Abstain  \n",
       "25209         -0.838545                           Abstain  \n",
       "27516         -0.607304                            Jewish  \n",
       "86434          0.000000                          Hispanic  \n",
       "4487           0.000000                          Hispanic  \n",
       "11562         -0.604397                     Blackamerican  \n",
       "83412        -10.000000                           Abstain  \n",
       "77012         -1.566677                           Abstain  \n",
       "81044         -0.700196                     West European  \n",
       "87601         -0.478807                            Jewish  \n",
       "36485         -1.274100                           Abstain  \n",
       "67015          0.000000                          Hispanic  \n",
       "47065         -1.295834                           Abstain  \n",
       "15569          0.000000                          Hispanic  \n",
       "83957         -1.372576                           Abstain  \n",
       "33917         -1.277361                           Abstain  \n",
       "57847         -0.939497                           Abstain  \n",
       "82310         -1.027081                           Abstain  \n",
       "6941          -1.391841                           Abstain  \n",
       "67531         -1.529831                           Abstain  \n",
       "1999          -1.057007                           Abstain  \n",
       "69095         -0.960518                           Abstain  \n",
       "65119         -0.973180                           Abstain  \n",
       "79989         -0.490454                     Blackamerican  \n",
       "23261          0.000000                          Hispanic  \n",
       "93019         -1.685518                           Abstain  \n",
       "15214         -0.597038                            Jewish  \n",
       "38586         -0.211128                     Blackamerican  \n",
       "27441          0.000000                   American Indian  \n",
       "...                 ...                               ...  \n",
       "47703         -1.117419                           Abstain  \n",
       "66827         -1.045815                           Abstain  \n",
       "91207          0.000000                          Hispanic  \n",
       "36392         -0.664569                     West European  \n",
       "3019          -1.317547                           Abstain  \n",
       "10378         -1.448828                           Abstain  \n",
       "25314         -0.912935                           Abstain  \n",
       "64848         -1.634483                           Abstain  \n",
       "21850         -1.239834                           Abstain  \n",
       "49218         -0.510897                     East European  \n",
       "31921         -0.783272                           Abstain  \n",
       "60812         -0.791560                           Abstain  \n",
       "15478        -10.000000                           Abstain  \n",
       "82349         -1.167648                           Abstain  \n",
       "70790         -1.540298                           Abstain  \n",
       "60803         -1.428192                           Abstain  \n",
       "92023        -10.000000                           Abstain  \n",
       "43832         -1.263331                           Abstain  \n",
       "55615          0.000000                          Hispanic  \n",
       "86878         -1.445590                           Abstain  \n",
       "99286         -0.253762                     Blackamerican  \n",
       "72053         -1.129779                           Abstain  \n",
       "82666         -0.123954                     Blackamerican  \n",
       "7630          -0.560999                     Blackamerican  \n",
       "1577           0.000000                          Hispanic  \n",
       "12857          0.000000                          Hispanic  \n",
       "90992          0.000000                          Hispanic  \n",
       "69098          0.000000                          Hispanic  \n",
       "47722         -1.218038                           Abstain  \n",
       "7718          -0.910811                           Abstain  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voters.sample(100)[['LAST_NAME', 'Race (Prior)','Ethnicity Prediction', \n",
    "                       'Prediction Score', 'Conservative Ethnicity Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "West European          31623\n",
       "Jewish                 22978\n",
       "Hispanic               16181\n",
       "Blackamerican           8507\n",
       "East European           5043\n",
       "Arab Muslim             4491\n",
       "Abstain                 4467\n",
       "Continental African     3680\n",
       "Black Muslim             801\n",
       "East Asian               761\n",
       "Indian Subcont.          747\n",
       "Asian Muslim             405\n",
       "American Indian          316\n",
       "Name: Ethnicity Prediction, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voters['Ethnicity Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, it seems that some minorities (like East Asians and American Indians) are very underrepresented and others (like West Europeans and Hispanics) are over represented. This seems to be in line, more or less, with our intuitions about the Florida populace.\n",
    "\n",
    "We are predicting a lot of Jewish people (around 1/5 of the population). This is unlikely given the composition of Florida.\n",
    "\n",
    "Below we have a more conservative model, that abstains when the threshold of -.75 Negative Log Likelihood is not met:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abstain                58144\n",
       "Hispanic               16181\n",
       "West European           8635\n",
       "Blackamerican           6191\n",
       "Jewish                  5895\n",
       "Continental African     1723\n",
       "East European            869\n",
       "Arab Muslim              566\n",
       "Indian Subcont.          521\n",
       "East Asian               472\n",
       "American Indian          316\n",
       "Black Muslim             261\n",
       "Asian Muslim             226\n",
       "Name: Conservative Ethnicity Prediction, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voters['Conservative Ethnicity Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the count for Jewish names went down to 5000 (from 22000). This indicates many of those predictions were predictions that our classifier was less confident about.\n",
    "\n",
    "The Conservative Model will be used as our \"Revised Ethnicity model\" for predictions into our Voter Records. Our Baseline Model will remain the \"Old Ethnicity model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's throw these imputations into a CSV to save it. This model took 3 hours to train, so don't want to mess up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_voters.to_csv('updated_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
