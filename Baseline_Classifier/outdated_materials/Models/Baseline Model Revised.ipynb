{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline Classifier Evaluation\n",
    "\n",
    "### Overview\n",
    "\n",
    "Here is our Baseline Model. Below, you will see design decisions we have made to construct this initial classifier, along with thoughts for potential improvements as we proceed in this final project.\n",
    "\n",
    "\n",
    "\n",
    "### TOC\n",
    "\n",
    "1. Import & Clean Name Data\n",
    "2. EDA on Name Data\n",
    "3. Training the Baseline Model\n",
    "4. Evaluating the Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ethnicityguesser.NLTKMaxentEthnicityClassifier import NLTKMaxentEthnicityClassifier as mxec\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import & Clean Name Data\n",
    "\n",
    "### Choosing the Data\n",
    "\n",
    "There are a couple of ways to get lists of names paired with ethnicity. Some solutions include:\n",
    "\n",
    "- **Lists of baby names** by ethnicity [familyeducation.com](http://familyeducation.com/baby-names/browse-origin/surname)\n",
    "- **Wikipedia Metadata** [Ethnicity List Scraped from Wikipedia](https://raw.githubusercontent.com/appeler/ethnicolr/master/ethnicolr/data/wiki/wiki_name_race.csv)\n",
    "- **Census Data** [Frequencies of names by race](https://raw.githubusercontent.com/appeler/ethnicolr/master/ethnicolr/data/census/census_2010.csv)\n",
    "\n",
    "For our base model, we chose the lists of baby names. Strengths of this data source include:\n",
    "- More specific ethnicity breakdowns (i.e. we can differentiate between Indian names vs Chinese names vs Vietnamese names, whereas the Census data would just tell us \"Asian\" because it is by race)\n",
    "- Standard use of 26 character alphabet (Wikipedia has many special characters like \"ä\" or \"ü\" on many names). Standard use is important because voter datasets do not use these special characters.\n",
    "\n",
    "Weaknesses of the baby names data source include:\n",
    "- No way to tell the \"frequency\" of a name\n",
    "- Exclusion of less common names not associated with ethnicity\n",
    "\n",
    "Considering the strengths and weaknesses of the baby names data, it seems like a reasonable starting point for our baseline model, although we may seek to refine future models through frequency data available from the other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import names paired with ethnicities ##\n",
    "\n",
    "# find names of files\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"ethnicityguesser/pickled_names\"):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "# list types of ethnicities\n",
    "ethnicities = []\n",
    "for each in f:\n",
    "    ethnicities.append(each.partition('.')[0])\n",
    "\n",
    "\n",
    "# pair type of ethnicity to its names in a dict\n",
    "eth_dict = {}\n",
    "for ethnicity in ethnicities:\n",
    "    with open('ethnicityguesser/pickled_names/'+ethnicity+'.pkl', 'rb') as filename:\n",
    "        names = pickle.load(filename)\n",
    "    eth_dict[ethnicity] = names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the ethnicities we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese',\n",
       " 'vietnamese',\n",
       " 'irish',\n",
       " 'danish',\n",
       " 'french',\n",
       " 'russian',\n",
       " 'japanese',\n",
       " 'german',\n",
       " 'czech',\n",
       " 'arabic',\n",
       " 'ukranian',\n",
       " 'swedish',\n",
       " 'spanish',\n",
       " 'african',\n",
       " 'swiss',\n",
       " 'korean',\n",
       " 'jewish',\n",
       " 'greek',\n",
       " 'italian',\n",
       " 'slavic',\n",
       " 'indian',\n",
       " 'muslim',\n",
       " 'portugese']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnicities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets package these into a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a datafrome of names and true ethnicities\n",
    "\n",
    "super_list_names = []\n",
    "super_list_ethnicities = []\n",
    "\n",
    "for ethnicity in ethnicities:\n",
    "    name_list = eth_dict[ethnicity][0]\n",
    "    eth_list = []\n",
    "    for name in name_list:\n",
    "        eth_list.append(ethnicity)\n",
    "    super_list_names = super_list_names + name_list\n",
    "    super_list_ethnicities = super_list_ethnicities + eth_list\n",
    "    \n",
    "df = pd.DataFrame(\n",
    "            {'Name': super_list_names,\n",
    "             'True Ethnicity': super_list_ethnicities\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA on Name Data\n",
    "\n",
    "Let's examine what our name data looks like in reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9458</th>\n",
       "      <td>Grinberg</td>\n",
       "      <td>swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>Herda</td>\n",
       "      <td>czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19557</th>\n",
       "      <td>Caro</td>\n",
       "      <td>portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13215</th>\n",
       "      <td>Guell</td>\n",
       "      <td>swiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>Grunder</td>\n",
       "      <td>swiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Bonet</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13994</th>\n",
       "      <td>Awerbuch</td>\n",
       "      <td>jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17392</th>\n",
       "      <td>Agli</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11597</th>\n",
       "      <td>Mejias</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>Chabot</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name True Ethnicity\n",
       "9458   Grinberg        swedish\n",
       "7489      Herda          czech\n",
       "19557      Caro      portugese\n",
       "13215     Guell          swiss\n",
       "13208   Grunder          swiss\n",
       "2000      Bonet         french\n",
       "13994  Awerbuch         jewish\n",
       "17392      Agli        italian\n",
       "11597    Mejias        spanish\n",
       "2349     Chabot         french"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the data we have for every ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n for each ethnicity sample\n",
      "426 chinese\n",
      "129 vietnamese\n",
      "318 irish\n",
      "656 danish\n",
      "4143 french\n",
      "181 russian\n",
      "566 japanese\n",
      "723 german\n",
      "1406 czech\n",
      "108 arabic\n",
      "409 ukranian\n",
      "1158 swedish\n",
      "2366 spanish\n",
      "300 african\n",
      "774 swiss\n",
      "169 korean\n",
      "3076 jewish\n",
      "429 greek\n",
      "711 italian\n",
      "258 slavic\n",
      "580 indian\n",
      "525 muslim\n",
      "834 portugese\n"
     ]
    }
   ],
   "source": [
    "print \"n for each ethnicity sample\"\n",
    "for ethnicity in ethnicities:\n",
    "    print len(df[df['True Ethnicity']==ethnicity]), ethnicity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Non-balanced\n",
    "\n",
    "We only have 169 Korean names, and 4143 French names. If we train a classifier on this data right off the bat, then we'll have something that may be unfairly biased towards French names.\n",
    "\n",
    "Frequency in this dataset does not correlate to real world frequency, so we ought to atleast balance it to prevent unwanted bias.\n",
    "\n",
    "### Problem 2: Too many categories\n",
    "\n",
    "Choosing between 23 specific ethnicities accurately is more difficult than choosing between around 7 or 8 broadly defined ethnicities because:\n",
    "  1. Having more choices to choose from in general creates more opportunities for classification errors\n",
    "  2. Some ethnicities from similar parts of the world have overlapping names (like \"Alexander\" is a common Danish, Greek, and French name).\n",
    "\n",
    "Let's consolidate some groups that share name/cultural similarities. Let's also make equal sample sizes for each consolidated group, drawing evenly from each subgroup to prevent our classifier from ignoring \"low frequency\" ethnicities that are only low frequency due to the bias in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample size: 7994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>Chinweike</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Phuong</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12780</th>\n",
       "      <td>Monifa</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18364</th>\n",
       "      <td>Bhalla</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19354</th>\n",
       "      <td>Shehata</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6904</th>\n",
       "      <td>Regenbogen</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>Son</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Tieu</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19266</th>\n",
       "      <td>Rashed</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12807</th>\n",
       "      <td>Nsia</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16249</th>\n",
       "      <td>Seckel</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11390</th>\n",
       "      <td>Limes</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18443</th>\n",
       "      <td>Deshmukh</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13776</th>\n",
       "      <td>Pyun</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8645</th>\n",
       "      <td>Shamoun</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name    True Ethnicity\n",
       "12676   Chinweike           African\n",
       "513        Phuong        East Asian\n",
       "12780      Monifa           African\n",
       "18364      Bhalla            Indian\n",
       "19354     Shehata     Muslim/Arabic\n",
       "6904   Regenbogen  Western European\n",
       "13800         Son        East Asian\n",
       "530          Tieu        East Asian\n",
       "19266      Rashed     Muslim/Arabic\n",
       "12807        Nsia           African\n",
       "16249      Seckel            Jewish\n",
       "11390       Limes          Hispanic\n",
       "18443    Deshmukh            Indian\n",
       "13776        Pyun        East Asian\n",
       "8645      Shamoun     Muslim/Arabic"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consolidation function\n",
    "\n",
    "df_c = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "\n",
    "def consolidate(eth_list, target_df, consolidated_eth):\n",
    "    sample_df = pd.DataFrame(columns=['Name', 'True Ethnicity'])\n",
    "    for ethnicity in eth_list:\n",
    "        eth_df = df[df['True Ethnicity']==ethnicity]\n",
    "        n_per_eth = (1000 / len(eth_list))\n",
    "        sample_df = pd.concat([sample_df, eth_df.sample(n=n_per_eth, replace = True)])\n",
    "    sample_df['True Ethnicity'] = consolidated_eth\n",
    "    return pd.concat([target_df,sample_df]) \n",
    "\n",
    "\n",
    "# Consolidate East European\n",
    "east_euro = ['russian','ukranian','czech','slavic']\n",
    "df_c = consolidate(east_euro, df_c, 'Eastern European')\n",
    "\n",
    "# Consolidate West European\n",
    "west_euro = ['italian','irish','danish','french',\n",
    "                'swedish','german','swiss']\n",
    "df_c = consolidate(west_euro, df_c, 'Western European')\n",
    "\n",
    "# Consolidate Muslim / Arab\n",
    "muslim_arabic = ['muslim', 'arabic']\n",
    "df_c = consolidate(muslim_arabic, df_c, 'Muslim/Arabic')\n",
    "\n",
    "# Consolidate East Asian\n",
    "east_asian = ['chinese','japanese','vietnamese','korean']\n",
    "df_c = consolidate(east_asian, df_c, 'East Asian')\n",
    "\n",
    "# Spanish / Hispanic can remain its own category\n",
    "hispanic = ['spanish','portugese'] \n",
    "df_c = consolidate(hispanic, df_c, 'Hispanic')\n",
    "\n",
    "# Jewish can remain its own category\n",
    "jewish = ['jewish']\n",
    "df_c = consolidate(jewish, df_c, 'Jewish')\n",
    "\n",
    "# Indian can remain its own category\n",
    "indian = ['indian']\n",
    "df_c = consolidate(indian, df_c, 'Indian')\n",
    "\n",
    "# African can remain its own category \n",
    "african = ['african']\n",
    "df_c = consolidate(african, df_c, 'African')\n",
    "\n",
    "print 'Cleaned sample size:', len(df_c)\n",
    "df_c.sample(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a lot of academic debate about how ethnicity categories ought to be defined, but for our purposes, lets just try to consildate groups based on similarities in names and culture of their American diaspora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model\n",
    "\n",
    "The Baseline Model uses a multinomial logistic regression (also known as a MaxEnt / Maximum Entropy classifier).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Because the classifier has many ethnicities it can classify into (it's not just a binary decision), a multinomial logistic regression is appropriate as it can classify into several non-ordinal categorical dependent variables.\n",
    "\n",
    "This model is particularly useful because we can output a probability for our prediction, that becomes an indicator for how certain we are about our classification. A future iteration of model can choose to \"abstain\" from predicting when uncertain, thereby giving us an extremely accurate list voters for a certain ethnicity if we choose a conservative threshold.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "For the baseline model, we adapted an open source implementation of a Max Entropy classifier prepared by Github user Kitofans, who has created a wrapper around the actual NLTK Max Entropy algorithm to take in names as training features.\n",
    "\n",
    "During training, the NLTK classifier considers all probability distributions that are consistent with the training data that has been fed in, and it chooses the distribution with the highest entropy.\n",
    "\n",
    "\n",
    "** Citation **\n",
    "\n",
    "Kitofan's wrapper here: https://github.com/kitofans/ethnicityguesser\n",
    "NLTK MaxEnt model: http://www.nltk.org/api/nltk.classify.html\n",
    "\n",
    "\n",
    "Now let's train this bad boy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample (n) 7994\n",
      "Test Sample (test n) 3894\n",
      "Train Sample (train n) 4100\n"
     ]
    }
   ],
   "source": [
    "## Split into Training and Test\n",
    "msk = np.random.rand(len(df_c)) < 0.5\n",
    "train_df = df_c[msk]\n",
    "test_df = df_c[~msk]\n",
    "\n",
    "print \"Total Sample (n)\", len (df_c)\n",
    "print \"Test Sample (test n)\", len(test_df)\n",
    "print \"Train Sample (train n)\", len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## List Consolidated Ethnicities\n",
    "ethnicities_c = [\n",
    "    'Eastern European',\n",
    "    'Western European',\n",
    "    'Muslim/Arabic',\n",
    "    'East Asian',\n",
    "    'Hispanic',\n",
    "    'Jewish',\n",
    "    'Indian',\n",
    "    'African'\n",
    "]\n",
    "\n",
    "## Package DF into training token\n",
    "train_tokens = []\n",
    "for ethnicity in ethnicities_c:\n",
    "    new_tokens = (list(train_df[train_df['True Ethnicity'] == ethnicity]['Name']), ethnicity)\n",
    "    train_tokens.append(new_tokens)\n",
    "\n",
    "# (Tokens must be a list of ([list of names], 'ethnicity') pairs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.07944        0.122\n",
      "             2          -1.14179        0.929\n",
      "             3          -0.78956        0.950\n",
      "             4          -0.61208        0.966\n",
      "             5          -0.50362        0.977\n",
      "             6          -0.42959        0.982\n",
      "             7          -0.37544        0.986\n",
      "             8          -0.33392        0.988\n",
      "             9          -0.30099        0.990\n",
      "            10          -0.27420        0.990\n",
      "            11          -0.25196        0.991\n",
      "            12          -0.23319        0.991\n",
      "            13          -0.21713        0.991\n",
      "            14          -0.20324        0.991\n",
      "            15          -0.19109        0.991\n",
      "            16          -0.18039        0.992\n",
      "            17          -0.17088        0.992\n",
      "            18          -0.16238        0.992\n",
      "            19          -0.15473        0.992\n",
      "            20          -0.14781        0.992\n",
      "            21          -0.14153        0.992\n",
      "            22          -0.13579        0.992\n",
      "            23          -0.13054        0.992\n",
      "            24          -0.12570        0.992\n",
      "            25          -0.12124        0.992\n",
      "            26          -0.11711        0.992\n",
      "            27          -0.11328        0.992\n",
      "            28          -0.10971        0.992\n",
      "            29          -0.10638        0.992\n",
      "            30          -0.10326        0.992\n",
      "            31          -0.10034        0.992\n",
      "            32          -0.09760        0.992\n",
      "            33          -0.09502        0.992\n",
      "            34          -0.09258        0.992\n",
      "            35          -0.09028        0.992\n",
      "            36          -0.08811        0.992\n",
      "            37          -0.08605        0.992\n",
      "            38          -0.08410        0.992\n",
      "            39          -0.08224        0.992\n",
      "            40          -0.08048        0.992\n",
      "            41          -0.07879        0.992\n",
      "            42          -0.07719        0.992\n",
      "            43          -0.07566        0.992\n",
      "            44          -0.07420        0.992\n",
      "            45          -0.07280        0.992\n",
      "            46          -0.07146        0.992\n",
      "            47          -0.07018        0.992\n",
      "            48          -0.06895        0.992\n",
      "            49          -0.06777        0.992\n",
      "            50          -0.06663        0.992\n",
      "            51          -0.06554        0.992\n",
      "            52          -0.06449        0.992\n",
      "            53          -0.06348        0.992\n",
      "            54          -0.06250        0.992\n",
      "            55          -0.06156        0.992\n",
      "            56          -0.06065        0.992\n",
      "            57          -0.05978        0.992\n",
      "            58          -0.05893        0.992\n",
      "            59          -0.05811        0.992\n",
      "            60          -0.05732        0.992\n",
      "            61          -0.05656        0.992\n",
      "            62          -0.05581        0.992\n",
      "            63          -0.05510        0.992\n",
      "            64          -0.05440        0.992\n",
      "            65          -0.05372        0.992\n",
      "            66          -0.05307        0.992\n",
      "            67          -0.05243        0.992\n",
      "            68          -0.05181        0.992\n",
      "            69          -0.05121        0.992\n",
      "            70          -0.05063        0.992\n",
      "            71          -0.05006        0.992\n",
      "            72          -0.04951        0.992\n",
      "            73          -0.04898        0.992\n",
      "            74          -0.04845        0.992\n",
      "            75          -0.04795        0.992\n",
      "            76          -0.04745        0.992\n",
      "            77          -0.04697        0.992\n",
      "            78          -0.04650        0.992\n",
      "            79          -0.04604        0.992\n",
      "            80          -0.04559        0.992\n",
      "            81          -0.04515        0.992\n",
      "            82          -0.04473        0.992\n",
      "            83          -0.04431        0.992\n",
      "            84          -0.04390        0.992\n",
      "            85          -0.04351        0.992\n",
      "            86          -0.04312        0.992\n",
      "            87          -0.04274        0.992\n",
      "            88          -0.04237        0.992\n",
      "            89          -0.04201        0.992\n",
      "            90          -0.04165        0.992\n",
      "            91          -0.04131        0.992\n",
      "            92          -0.04097        0.992\n",
      "            93          -0.04063        0.992\n",
      "            94          -0.04031        0.992\n",
      "            95          -0.03999        0.992\n",
      "            96          -0.03968        0.992\n",
      "            97          -0.03937        0.992\n",
      "            98          -0.03907        0.992\n",
      "            99          -0.03878        0.992\n",
      "         Final          -0.03849        0.992\n"
     ]
    }
   ],
   "source": [
    "## Train Classifier (beware, this takes time)\n",
    "\n",
    "classifier = mxec(train_tokens)\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael      Western European\n",
      "Roberto      Hispanic\n",
      "Lee          East Asian\n",
      "sajkfldsafh  Muslim/Arabic\n"
     ]
    }
   ],
   "source": [
    "# Test Classifier\n",
    "print \"Michael     \", classifier.classify('Michael')\n",
    "print \"Roberto     \", classifier.classify('Roberto')\n",
    "print \"Lee         \", classifier.classify('Lee')\n",
    "print \"sajkfldsafh \", classifier.classify('sajkfldsafh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions look reasonable. Let's take a peek under the hood and see what the probabilities of the predictions are like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Eastern European', -3.7065529482840653), ('Jewish', -6.6514402039659508), ('Western European', -0.16855233603485154), ('African', -11.308063175888986), ('Hispanic', -7.7907496492955053), ('Indian', -8.3545578338112048), ('East Asian', -6.0727470332155962), ('Muslim/Arabic', -10.121878347922477)]\n"
     ]
    }
   ],
   "source": [
    "def prob(name):\n",
    "    return classifier.prob_classify(name)._prob_dict.items()\n",
    "    #return max((p,v) for (v,p) in classifier.prob_classify(name)._prob_dict.items())\n",
    "\n",
    "# find probability of prediction as log (lower is better)\n",
    "print prob('Michael')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to documentation, the classifier chooses whichever ethnicity has the highest score here. For \"Michael\", that would be:\n",
    "* 'Western European', -0.16855233603485154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>True Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Triebel</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>Hakimi</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>Alpron</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Apollo</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>Baria</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>Shankar</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>Zimbalist</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>Mifsud</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>Muraoka</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Acconci</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Chernoff</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Petrik</td>\n",
       "      <td>Eastern European</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Magro</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>Kwabena</td>\n",
       "      <td>African</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Mccormick</td>\n",
       "      <td>Western European</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name        Prediction    True Ethnicity\n",
       "419     Triebel  Western European  Eastern European\n",
       "1386     Hakimi     Muslim/Arabic     Muslim/Arabic\n",
       "2639     Alpron  Western European            Jewish\n",
       "1978     Apollo          Hispanic          Hispanic\n",
       "2899      Baria          Hispanic            Indian\n",
       "3379    Shankar            Indian            Indian\n",
       "2741  Zimbalist            Jewish            Jewish\n",
       "1314     Mifsud     Muslim/Arabic     Muslim/Arabic\n",
       "1589    Muraoka        East Asian        East Asian\n",
       "479     Acconci  Western European  Western European\n",
       "99     Chernoff            Jewish  Eastern European\n",
       "172      Petrik  Eastern European  Eastern European\n",
       "1955      Magro          Hispanic          Hispanic\n",
       "3786    Kwabena           African           African\n",
       "563   Mccormick  Western European  Western European"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict!!!!\n",
    "\n",
    "test_names = list(test_df['Name'])\n",
    "test_eth = list(test_df['True Ethnicity'])\n",
    "\n",
    "test_preds = []\n",
    "\n",
    "for name in test_names:\n",
    "    pred = classifier.classify(name)\n",
    "    test_preds.append(pred)\n",
    "\n",
    "df_preds = pd.DataFrame({\n",
    "    'Name': test_names,\n",
    "    'True Ethnicity': test_eth,\n",
    "    'Prediction': test_preds\n",
    "})\n",
    "\n",
    "df_preds.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks alright. Let's add an indicator variable of True if the guess is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>De Araujo</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>Siddiqui</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Tzarfat</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>Chazzan</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Urban</td>\n",
       "      <td>Eastern European</td>\n",
       "      <td>Eastern European</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>Atiyeh</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>Upadhyay</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>Sundaram</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Oluwatoyin</td>\n",
       "      <td>African</td>\n",
       "      <td>African</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>Kohen</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Kim</td>\n",
       "      <td>Muslim/Arabic</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>Marcelino</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>Velez</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>Ray</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name        Prediction    True Ethnicity  Accuracy\n",
       "1368       Koury     Muslim/Arabic     Muslim/Arabic      True\n",
       "2245   De Araujo          Hispanic          Hispanic      True\n",
       "1092    Siddiqui     Muslim/Arabic     Muslim/Arabic      True\n",
       "2399     Tzarfat            Jewish            Jewish      True\n",
       "2872     Chazzan            Jewish            Jewish      True\n",
       "113        Urban  Eastern European  Eastern European      True\n",
       "1345      Atiyeh     Muslim/Arabic     Muslim/Arabic      True\n",
       "3146    Upadhyay            Indian            Indian      True\n",
       "3215    Sundaram            Jewish            Indian     False\n",
       "3801  Oluwatoyin           African           African      True\n",
       "2625       Kohen            Jewish            Jewish      True\n",
       "1862         Kim     Muslim/Arabic        East Asian     False\n",
       "2308   Marcelino          Hispanic          Hispanic      True\n",
       "2249       Velez          Hispanic          Hispanic      True\n",
       "3090         Ray            Indian            Indian      True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add True if you got it right\n",
    "df_preds['Accuracy'] = (df_preds['Prediction']==df_preds['True Ethnicity'])\n",
    "df_preds.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the Model\n",
    "\n",
    "With logit regressions, the ROC curve and corresponding AUC are usually great ways to assess overall accuracy, but this is not a binary classification type of problem. We'll instead use evaluation metrics like:\n",
    "- Classification Accuracy\n",
    "- TPR\n",
    "- FPR\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score (harmonic mean of precision and recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tools to Calculate One vs. Rest Accuracy Rates\n",
    "\n",
    "def calcTP(df, eth):\n",
    "    P = df[df['Prediction']==eth]\n",
    "    TP = P[P['True Ethnicity']==eth]\n",
    "    return len(TP)\n",
    "\n",
    "def calcFP(df, eth):\n",
    "    P = df[df['Prediction']==eth]\n",
    "    FP = P[P['True Ethnicity']!=eth]\n",
    "    return len(FP)\n",
    "\n",
    "def calcTN(df, eth):\n",
    "    N = df[df['Prediction']!=eth]\n",
    "    TN = N[N['True Ethnicity']!=eth]\n",
    "    return len(TN)\n",
    "\n",
    "def calcFN(df, eth):\n",
    "    N = df[df['Prediction']!=eth]\n",
    "    FN = N[N['True Ethnicity']==eth]\n",
    "    return len(FN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "TPs = [] # number of times predict X when  X ethnicity\n",
    "FPs = [] # number of times predict X when 'X ethnicity\n",
    "TNs = [] # number of times predict'X when 'X ethnicity\n",
    "FNs = [] # number of times predict'X when  X ethnicity\n",
    "\n",
    "\n",
    "ethnicity_list = []\n",
    "\n",
    "# Classification Accuracy\n",
    "for ethnicity in ethnicities_c:\n",
    "    accuracy = calcAccuracy(df_preds[df_preds['True Ethnicity']==ethnicity])\n",
    "    accuracies.append(accuracy)\n",
    "    TPs.append(calcTP(df_preds, ethnicity))\n",
    "    FPs.append(calcFP(df_preds, ethnicity))\n",
    "    TNs.append(calcTN(df_preds, ethnicity))\n",
    "    FNs.append(calcFN(df_preds, ethnicity))\n",
    "    ethnicity_list.append(ethnicity)\n",
    "\n",
    "# Aggregate accuracy\n",
    "#accuracies.append(calcAccuracy(df_preds))\n",
    "#ethnicity_list.append('OVERALL')\n",
    "\n",
    "# put into df\n",
    "df_acc = pd.DataFrame({\n",
    "    'True Ethnicity': ethnicity_list,\n",
    "    'Classification Accuracy': accuracies,\n",
    "    'TP': TPs,\n",
    "    'FP': FPs,\n",
    "    'TN': TNs,\n",
    "    'FN': FNs\n",
    "})\n",
    "\n",
    "df_acc.set_index('True Ethnicity', inplace=True)\n",
    "\n",
    "# Add TPR (Sensistivity)\n",
    "df_acc['Sensitivity (TPR)'] = (df_acc['TP']) / (df_acc['TP'] + df_acc['FN'])\n",
    "\n",
    "# Add FPR\n",
    "df_acc['FPR'] = (df_acc['FP']) / (df_acc['FP'] + df_acc['TN'])\n",
    "\n",
    "# Add Precision\n",
    "df_acc['Precision'] = (df_acc['TP']) / (df_acc['TP'] + df_acc['FP'])\n",
    "\n",
    "# F1 Score (harmonic mean of precision and sensitivity)\n",
    "df_acc['F1 Score'] = (2 * df_acc['TP'] / ((2*df_acc['TP'])+df_acc['FP']+df_acc['FN']))\n",
    "\n",
    "# Accuracy (ACC)\n",
    "df_acc['ACC'] = (df_acc['TP']+df_acc['TN']) / (df_acc['TP']+df_acc['TN']+df_acc['FP']+df_acc['FN'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Accuracy</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity (TPR)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eastern European</th>\n",
       "      <td>0.737418</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>3307</td>\n",
       "      <td>337</td>\n",
       "      <td>0.737418</td>\n",
       "      <td>0.037824</td>\n",
       "      <td>0.721627</td>\n",
       "      <td>0.729437</td>\n",
       "      <td>0.935799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western European</th>\n",
       "      <td>0.577236</td>\n",
       "      <td>208</td>\n",
       "      <td>174</td>\n",
       "      <td>3228</td>\n",
       "      <td>284</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.051146</td>\n",
       "      <td>0.620087</td>\n",
       "      <td>0.597895</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Muslim/Arabic</th>\n",
       "      <td>0.852590</td>\n",
       "      <td>74</td>\n",
       "      <td>123</td>\n",
       "      <td>3269</td>\n",
       "      <td>428</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.776770</td>\n",
       "      <td>0.812915</td>\n",
       "      <td>0.949409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>East Asian</th>\n",
       "      <td>0.789130</td>\n",
       "      <td>97</td>\n",
       "      <td>96</td>\n",
       "      <td>3338</td>\n",
       "      <td>363</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.790850</td>\n",
       "      <td>0.789989</td>\n",
       "      <td>0.950437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>0.776639</td>\n",
       "      <td>109</td>\n",
       "      <td>138</td>\n",
       "      <td>3268</td>\n",
       "      <td>379</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.040517</td>\n",
       "      <td>0.733075</td>\n",
       "      <td>0.754229</td>\n",
       "      <td>0.936569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewish</th>\n",
       "      <td>0.520243</td>\n",
       "      <td>237</td>\n",
       "      <td>160</td>\n",
       "      <td>3240</td>\n",
       "      <td>257</td>\n",
       "      <td>0.520243</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.616307</td>\n",
       "      <td>0.564215</td>\n",
       "      <td>0.898048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.760784</td>\n",
       "      <td>122</td>\n",
       "      <td>113</td>\n",
       "      <td>3271</td>\n",
       "      <td>388</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>0.774451</td>\n",
       "      <td>0.767557</td>\n",
       "      <td>0.939651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.947047</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>3344</td>\n",
       "      <td>465</td>\n",
       "      <td>0.947047</td>\n",
       "      <td>0.017338</td>\n",
       "      <td>0.887405</td>\n",
       "      <td>0.916256</td>\n",
       "      <td>0.978172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Classification Accuracy   FN   FP    TN   TP  \\\n",
       "True Ethnicity                                                   \n",
       "Eastern European                 0.737418  120  130  3307  337   \n",
       "Western European                 0.577236  208  174  3228  284   \n",
       "Muslim/Arabic                    0.852590   74  123  3269  428   \n",
       "East Asian                       0.789130   97   96  3338  363   \n",
       "Hispanic                         0.776639  109  138  3268  379   \n",
       "Jewish                           0.520243  237  160  3240  257   \n",
       "Indian                           0.760784  122  113  3271  388   \n",
       "African                          0.947047   26   59  3344  465   \n",
       "\n",
       "                  Sensitivity (TPR)       FPR  Precision  F1 Score       ACC  \n",
       "True Ethnicity                                                                \n",
       "Eastern European           0.737418  0.037824   0.721627  0.729437  0.935799  \n",
       "Western European           0.577236  0.051146   0.620087  0.597895  0.901900  \n",
       "Muslim/Arabic              0.852590  0.036262   0.776770  0.812915  0.949409  \n",
       "East Asian                 0.789130  0.027956   0.790850  0.789989  0.950437  \n",
       "Hispanic                   0.776639  0.040517   0.733075  0.754229  0.936569  \n",
       "Jewish                     0.520243  0.047059   0.616307  0.564215  0.898048  \n",
       "Indian                     0.760784  0.033392   0.774451  0.767557  0.939651  \n",
       "African                    0.947047  0.017338   0.887405  0.916256  0.978172  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Our ACCs (Overall accuracy scores) are super high, and our FPRs are very low, but these are actually misleading because they are inflated by our high True Negative count. We are using a one vs. rest calculation approach. This calculation approach leads to a high number of true negatives, because most guesses will simply not be the \"one\" ethnicity in question.\n",
    "\n",
    "The most useful metrics here are probably True Positive Rate/Sensitivity (because it tells us how often we are predicting correctly within a given ethnicity), and the Precision (because it tells the likelihood of whether or not our guess is correct, once we make it).\n",
    "\n",
    "F1 Score is interesting because it gives us a harmonic mean of these two metrics (Precision an Sensitivity) so it can help us consider those two together.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "\n",
    "** Tuning this Model **\n",
    "Moving forward, we think that it would be beneficial to optimize for Precision (by only classifying as an ethnicity when we are sure, potentially abstaining in ambiguous cases). This would cause us only predict on some data points.\n",
    "\n",
    "In our research project, we think this is a good trade off. Given the enormous size of our dataset (>10 million voters in Florida), it is fine for us to abstain from predicting for many people. Even if we predict on only 1 million of the most ethnically identifiable names for each ethnicity, that is enough to make conclusions on the state level.\n",
    "\n",
    "One concern would be that be only classifying highly identifiable names, we are introducing a bias, because people of a certain ethnicity who have a less identifiable name may behave differently as voters. As a working assumption, we'll assume that the ethnic identifiability of person's name does not have a causal or confounding effect on their voting behavior, and that abstaining on those less identifiable names is better than making an incorrect classification.\n",
    "\n",
    "** Testing Other Models **\n",
    "This Multinomial Logistic Regression (MaxEnt) is one of many implementation that can be used to classify based on name. Other reasonable models could include:\n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- hidden Markov models\n",
    "- k-means clustering\n",
    "- LDA\n",
    "\n",
    "We will continue to explore the literature on name/language classification and will test one or more of the above models as appropriate.\n",
    "\n",
    "\n",
    "** Using Better Data **\n",
    "\n",
    "A limitation of our approach is that a certain name may appear more frequently in one ethnicity, and less in another, but our Baby Names dataset does not account for this.\n",
    "\n",
    "We will try to experiment with other data sources to see if we can get a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini EDA: Applying the Predictor to a subset of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>district</th>\n",
       "      <th>id</th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>FIRST_NAME</th>\n",
       "      <th>zip</th>\n",
       "      <th>female</th>\n",
       "      <th>dob</th>\n",
       "      <th>regyear</th>\n",
       "      <th>party</th>\n",
       "      <th>electiondate</th>\n",
       "      <th>general</th>\n",
       "      <th>typeofvote</th>\n",
       "      <th>age</th>\n",
       "      <th>GEN16</th>\n",
       "      <th>GEN14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>5139</td>\n",
       "      <td>5139</td>\n",
       "      <td>3153570</td>\n",
       "      <td>CLL</td>\n",
       "      <td>103047739</td>\n",
       "      <td>Howard</td>\n",
       "      <td>Timothy</td>\n",
       "      <td>34117</td>\n",
       "      <td>M</td>\n",
       "      <td>1961-10-26 00:00:00</td>\n",
       "      <td>09/22/2000</td>\n",
       "      <td>REP</td>\n",
       "      <td>11/06/2012</td>\n",
       "      <td>GEN</td>\n",
       "      <td>A</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>1443</td>\n",
       "      <td>1443</td>\n",
       "      <td>1696657</td>\n",
       "      <td>CHA</td>\n",
       "      <td>102606670</td>\n",
       "      <td>Pruey</td>\n",
       "      <td>R</td>\n",
       "      <td>34224</td>\n",
       "      <td>M</td>\n",
       "      <td>1980-01-11 00:00:00</td>\n",
       "      <td>05/06/1998</td>\n",
       "      <td>NPA</td>\n",
       "      <td>11/08/2016</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>4871</td>\n",
       "      <td>4871</td>\n",
       "      <td>5669616</td>\n",
       "      <td>DUV</td>\n",
       "      <td>103293240</td>\n",
       "      <td>Moore</td>\n",
       "      <td>William</td>\n",
       "      <td>32233</td>\n",
       "      <td>M</td>\n",
       "      <td>1956-11-13 00:00:00</td>\n",
       "      <td>10/08/1990</td>\n",
       "      <td>REP</td>\n",
       "      <td>11/08/2016</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>9672</td>\n",
       "      <td>9672</td>\n",
       "      <td>7433685</td>\n",
       "      <td>DUV</td>\n",
       "      <td>103841413</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>Sue</td>\n",
       "      <td>32210</td>\n",
       "      <td>U</td>\n",
       "      <td>1969-04-06 00:00:00</td>\n",
       "      <td>01/31/1995</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/06/2012</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>1608176</td>\n",
       "      <td>CHA</td>\n",
       "      <td>102626244</td>\n",
       "      <td>Kelley</td>\n",
       "      <td>Kristine</td>\n",
       "      <td>34224</td>\n",
       "      <td>F</td>\n",
       "      <td>1966-02-24 00:00:00</td>\n",
       "      <td>11/15/2000</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/07/2006</td>\n",
       "      <td>GEN</td>\n",
       "      <td>N</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>9887</td>\n",
       "      <td>9887</td>\n",
       "      <td>6220186</td>\n",
       "      <td>DUV</td>\n",
       "      <td>103752822</td>\n",
       "      <td>Carswell</td>\n",
       "      <td>Dana</td>\n",
       "      <td>32277</td>\n",
       "      <td>F</td>\n",
       "      <td>1952-09-01 00:00:00</td>\n",
       "      <td>11/08/1983</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/04/2014</td>\n",
       "      <td>GEN</td>\n",
       "      <td>E</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>3704</td>\n",
       "      <td>3704</td>\n",
       "      <td>2768859</td>\n",
       "      <td>CLL</td>\n",
       "      <td>103016985</td>\n",
       "      <td>James</td>\n",
       "      <td>Carol</td>\n",
       "      <td>34112</td>\n",
       "      <td>F</td>\n",
       "      <td>1939-06-15 00:00:00</td>\n",
       "      <td>02/26/1998</td>\n",
       "      <td>REP</td>\n",
       "      <td>11/07/2006</td>\n",
       "      <td>GEN</td>\n",
       "      <td>E</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>5248</td>\n",
       "      <td>5248</td>\n",
       "      <td>999427</td>\n",
       "      <td>BRA</td>\n",
       "      <td>100753930</td>\n",
       "      <td>Goodman</td>\n",
       "      <td>Anessa</td>\n",
       "      <td>32058</td>\n",
       "      <td>F</td>\n",
       "      <td>1975-07-07 00:00:00</td>\n",
       "      <td>05/05/1997</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/06/2012</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>5449</td>\n",
       "      <td>5449</td>\n",
       "      <td>3971893</td>\n",
       "      <td>CLM</td>\n",
       "      <td>103179402</td>\n",
       "      <td>DRAWDY</td>\n",
       "      <td>CAROLYN</td>\n",
       "      <td>32024</td>\n",
       "      <td>F</td>\n",
       "      <td>1941-07-16 00:00:00</td>\n",
       "      <td>03/28/1968</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/04/2008</td>\n",
       "      <td>GEN</td>\n",
       "      <td>E</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1595</td>\n",
       "      <td>1595</td>\n",
       "      <td>1290058</td>\n",
       "      <td>CHA</td>\n",
       "      <td>102560007</td>\n",
       "      <td>Goldman</td>\n",
       "      <td>Jason</td>\n",
       "      <td>33954</td>\n",
       "      <td>M</td>\n",
       "      <td>1970-03-23 00:00:00</td>\n",
       "      <td>06/02/1988</td>\n",
       "      <td>DEM</td>\n",
       "      <td>11/04/2014</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Y</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1    index district         id LAST_NAME  \\\n",
       "2644        5139          5139  3153570      CLL  103047739    Howard   \n",
       "754         1443          1443  1696657      CHA  102606670     Pruey   \n",
       "2516        4871          4871  5669616      DUV  103293240     Moore   \n",
       "4975        9672          9672  7433685      DUV  103841413   Sanders   \n",
       "34            70            70  1608176      CHA  102626244    Kelley   \n",
       "5081        9887          9887  6220186      DUV  103752822  Carswell   \n",
       "1910        3704          3704  2768859      CLL  103016985     James   \n",
       "2701        5248          5248   999427      BRA  100753930   Goodman   \n",
       "2801        5449          5449  3971893      CLM  103179402    DRAWDY   \n",
       "834         1595          1595  1290058      CHA  102560007   Goldman   \n",
       "\n",
       "     FIRST_NAME    zip female                  dob     regyear party  \\\n",
       "2644    Timothy  34117      M  1961-10-26 00:00:00  09/22/2000   REP   \n",
       "754           R  34224      M  1980-01-11 00:00:00  05/06/1998   NPA   \n",
       "2516    William  32233      M  1956-11-13 00:00:00  10/08/1990   REP   \n",
       "4975        Sue  32210      U  1969-04-06 00:00:00  01/31/1995   DEM   \n",
       "34     Kristine  34224      F  1966-02-24 00:00:00  11/15/2000   DEM   \n",
       "5081       Dana  32277      F  1952-09-01 00:00:00  11/08/1983   DEM   \n",
       "1910      Carol  34112      F  1939-06-15 00:00:00  02/26/1998   REP   \n",
       "2701     Anessa  32058      F  1975-07-07 00:00:00  05/05/1997   DEM   \n",
       "2801    CAROLYN  32024      F  1941-07-16 00:00:00  03/28/1968   DEM   \n",
       "834       Jason  33954      M  1970-03-23 00:00:00  06/02/1988   DEM   \n",
       "\n",
       "     electiondate general typeofvote   age  GEN16  GEN14  \n",
       "2644   11/06/2012     GEN          A  56.0      0      0  \n",
       "754    11/08/2016     GEN          Y  37.0      1      0  \n",
       "2516   11/08/2016     GEN          Y  61.0      1      0  \n",
       "4975   11/06/2012     GEN          Y  48.0      0      0  \n",
       "34     11/07/2006     GEN          N  51.0      0      0  \n",
       "5081   11/04/2014     GEN          E  65.0      0      0  \n",
       "1910   11/07/2006     GEN          E  78.0      0      0  \n",
       "2701   11/06/2012     GEN          Y  42.0      0      0  \n",
       "2801   11/04/2008     GEN          E  76.0      0      1  \n",
       "834    11/04/2014     GEN          Y  47.0      0      0  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import data cleaned by Riddhi and Kimia\n",
    "df_voters = pd.read_csv('Milestone33.csv', sep='\\t')\n",
    "df_voters.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add predictions to voter data\n",
    "ethnicity_predictions = []\n",
    "for name in list(df_voters['LAST_NAME']):\n",
    "    ethnicity_predictions.append(classifier.classify(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_voters['Ethnicity Prediction'] = ethnicity_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>Ethnicity Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>Kelleher</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>Pullen</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>Pineda</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>MCDOWELL</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>Lessord</td>\n",
       "      <td>Eastern European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>Clemons</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>Montgomery</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>THIBODAUX</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>RANDLES</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>THORNTON</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>Kopren</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>Prosper</td>\n",
       "      <td>Western European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>BEAM</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Jackson</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Torres</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LAST_NAME Ethnicity Prediction\n",
       "1526    Kelleher               Jewish\n",
       "3536      Pullen     Western European\n",
       "4547      Pineda             Hispanic\n",
       "4707    MCDOWELL     Western European\n",
       "2959     Lessord     Eastern European\n",
       "3846     Clemons     Western European\n",
       "3054  Montgomery     Western European\n",
       "4772   THIBODAUX     Western European\n",
       "1682     RANDLES             Hispanic\n",
       "3758    THORNTON     Western European\n",
       "4916      Kopren     Western European\n",
       "1937     Prosper     Western European\n",
       "108         BEAM               Jewish\n",
       "585      Jackson               Jewish\n",
       "452       Torres             Hispanic"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voters.sample(15)[['LAST_NAME', 'Ethnicity Prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Baseline looks reasonable on the voter data, but definitely has room for improvement."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
