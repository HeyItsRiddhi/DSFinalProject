{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 8\n",
    "# Ensemble methods\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kimia/Desktop/CS209/DSFinalProject/HW8\n"
     ]
    }
   ],
   "source": [
    "% cd ~/Desktop/CS209/DSFinalProject/HW8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higgs_test.csv            cs109-hw8.ipynb           wine_quality_missing.csv\r\n",
      "Higgs_train.csv           models.npy\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between a collision that produces Higgs bosons and collisions thats produce only background noise. We shall explore the use of ensemble methods for this classification task.\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle colision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background). \n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: <a href = \"https://www.nature.com/articles/ncomms5308\">Baldi et al., Nature Communications 5, 2014</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Higgs_train.csv')\n",
    "test = pd.read_csv('Higgs_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lepton pT', ' lepton eta', ' lepton phi', ' missing energy magnitude',\n",
       "       ' missing energy phi', ' jet 1 pt', ' jet 1 eta', ' jet 1 phi',\n",
       "       ' jet 1 b-tag', ' jet 2 pt', ' jet 2 eta', ' jet 2 phi', ' jet 2 b-tag',\n",
       "       ' jet 3 pt', ' jet 3 eta', ' jet 3 phi', ' jet 3 b-tag', ' jet 4 pt',\n",
       "       ' jet 4 eta', ' jet 4 phi', ' jet 4 b-tag', ' m_jj', ' m_jjj', ' m_lv',\n",
       "       ' m_jlv', ' m_bb', ' m_wbb', ' m_wwbb', ' class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split data into X and y\n",
    "y_train = train[' class']\n",
    "X_train = train[train.columns[train.columns != ' class']]\n",
    "y_test = test[' class']\n",
    "X_test = test[train.columns[train.columns != ' class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>...</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376816</td>\n",
       "      <td>-1.583727</td>\n",
       "      <td>-1.707552</td>\n",
       "      <td>0.990897</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>1.253553</td>\n",
       "      <td>0.619859</td>\n",
       "      <td>-1.479572</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.753658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522449</td>\n",
       "      <td>1.318622</td>\n",
       "      <td>0.982398</td>\n",
       "      <td>1.359610</td>\n",
       "      <td>0.964809</td>\n",
       "      <td>1.309991</td>\n",
       "      <td>1.083203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707330</td>\n",
       "      <td>0.087603</td>\n",
       "      <td>-0.399742</td>\n",
       "      <td>0.918742</td>\n",
       "      <td>-1.229936</td>\n",
       "      <td>1.172847</td>\n",
       "      <td>-0.552574</td>\n",
       "      <td>0.886053</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>1.298317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439696</td>\n",
       "      <td>0.828885</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>1.157820</td>\n",
       "      <td>2.215780</td>\n",
       "      <td>1.189586</td>\n",
       "      <td>0.937976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617290</td>\n",
       "      <td>0.265839</td>\n",
       "      <td>-1.345227</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>1.036646</td>\n",
       "      <td>0.954822</td>\n",
       "      <td>0.377252</td>\n",
       "      <td>-0.147960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.024506</td>\n",
       "      <td>1.026255</td>\n",
       "      <td>0.986289</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>1.371080</td>\n",
       "      <td>0.981672</td>\n",
       "      <td>0.917436</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850992</td>\n",
       "      <td>-0.380876</td>\n",
       "      <td>-0.071264</td>\n",
       "      <td>1.468704</td>\n",
       "      <td>-0.795133</td>\n",
       "      <td>0.691818</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.496881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.616349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.197755</td>\n",
       "      <td>1.100534</td>\n",
       "      <td>0.987262</td>\n",
       "      <td>1.353453</td>\n",
       "      <td>1.455383</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>0.953553</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.767540</td>\n",
       "      <td>-0.691572</td>\n",
       "      <td>-0.040191</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.143765</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.397057</td>\n",
       "      <td>-0.873640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.147862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502034</td>\n",
       "      <td>1.550981</td>\n",
       "      <td>0.921948</td>\n",
       "      <td>0.864080</td>\n",
       "      <td>0.982839</td>\n",
       "      <td>1.373222</td>\n",
       "      <td>0.601492</td>\n",
       "      <td>0.918621</td>\n",
       "      <td>0.957063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT   lepton eta   lepton phi   missing energy magnitude  \\\n",
       "0   0.376816    -1.583727    -1.707552                   0.990897   \n",
       "1   0.707330     0.087603    -0.399742                   0.918742   \n",
       "2   0.617290     0.265839    -1.345227                   1.154581   \n",
       "3   0.850992    -0.380876    -0.071264                   1.468704   \n",
       "4   0.767540    -0.691572    -0.040191                   0.614843   \n",
       "\n",
       "    missing energy phi   jet 1 pt   jet 1 eta   jet 1 phi   jet 1 b-tag  \\\n",
       "0             0.114397   1.253553    0.619859   -1.479572      2.173076   \n",
       "1            -1.229936   1.172847   -0.552574    0.886053      2.173076   \n",
       "2             1.036646   0.954822    0.377252   -0.147960      0.000000   \n",
       "3            -0.795133   0.691818    0.883260    0.496881      0.000000   \n",
       "4             0.143765   0.748614    0.397057   -0.873640      0.000000   \n",
       "\n",
       "    jet 2 pt   ...     jet 4 phi   jet 4 b-tag      m_jj     m_jjj      m_lv  \\\n",
       "0   0.753658   ...      0.397156      0.000000  0.522449  1.318622  0.982398   \n",
       "1   1.298317   ...      0.236231      0.000000  0.439696  0.828885  0.992241   \n",
       "2   1.063507   ...     -0.542413      0.000000  1.024506  1.026255  0.986289   \n",
       "3   1.616349   ...     -1.520171      0.000000  1.197755  1.100534  0.987262   \n",
       "4   1.147862   ...      0.502034      1.550981  0.921948  0.864080  0.982839   \n",
       "\n",
       "      m_jlv      m_bb     m_wbb    m_wwbb   class  \n",
       "0  1.359610  0.964809  1.309991  1.083203     1.0  \n",
       "1  1.157820  2.215780  1.189586  0.937976     1.0  \n",
       "2  0.927720  1.371080  0.981672  0.917436     1.0  \n",
       "3  1.353453  1.455383  0.994682  0.953553     1.0  \n",
       "4  1.373222  0.601492  0.918621  0.957063     0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Standardize or Normalize??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2pt): Single Decision Tree\n",
    "We start by building a basic model which we will use as our base model for comparison. \n",
    "\n",
    "1. Fit a decision tree model to the training set and report the classification accuracy of the model on the test set. Use 5-fold cross-validation to choose the (maximum) depth for the tree. You will use the max_depth you find here throughout the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYlFCUNERGL5f/z4A/Y5Ay5ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10adea6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "depth = []\n",
    "tree_start = 3\n",
    "tree_end   = 20\n",
    "for i in range(tree_start,tree_end):\n",
    "    dt = DecisionTreeClassifier(max_depth=i)\n",
    "    # Perform 5-fold cross validation \n",
    "    scores = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "    depth.append((i,scores.mean()))\n",
    "plt.plot(np.array(depth)[:,0], np.array(depth)[:,1], 'b-')\n",
    "plt.ylabel(\"Cross Validation Accuracy\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64559999999999995"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_depth = np.argmax(np.array(depth)[:,1]) + tree_start\n",
    "best_depth\n",
    "dt = DecisionTreeClassifier(max_depth=best_depth) \n",
    "dt_fitted = dt.fit(X_train, y_train) \n",
    "dt_fitted.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat=0</th>\n",
       "      <th>y_hat = 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>y=0</th>\n",
       "      <td>1528</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y=1</th>\n",
       "      <td>948</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_hat=0  y_hat = 1\n",
       "y=0     1528        824\n",
       "y=1      948       1700"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = y_test\n",
    "predicted = dt_fitted.predict(X_test)\n",
    "conf_mat = confusion_matrix(expected, predicted)\n",
    "conf_df = pd.DataFrame(conf_mat, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (15pt): Dropout-based Approach\n",
    "We start with a simple method inspired from the idea of 'dropout' in machine learning, where we fit multiple decision trees on random subsets of predictors, and combine them through a majority vote. The procedure is described below.\n",
    "\n",
    "- For each predictor in the training sample, set the predictor values to 0 with probability $p$  (i.e. drop the predictor by setting it to 0). Repeat this for $B$ trials to create $B$ separate training sets.\n",
    "\n",
    "\n",
    "- Fit decision tree models $\\hat{h}^1(x), \\ldots, \\hat{h}^B(x) \\in \\{0,1\\}$ to the $B$ training sets. \n",
    "\n",
    "- Combine the decision tree models into a single classifier by taking a majority vote:\n",
    "$$\n",
    "\\hat{H}_{maj}(x) \\,=\\, majority\\Big(\\hat{h}^1(x), \\ldots, \\hat{h}^B(x)\\Big).\n",
    "$$\n",
    "\n",
    "\n",
    "We shall refer to the combined classifier as an ** *ensemble classifier* **. Implement the described dropout approach, and answer the following questions:\n",
    "1. Apply the dropout procedure with $p = 0.5$ for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracy of the combined classifier. Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "- Fix the number of trees to 64 and apply the dropout procedure with different dropout rates $p = 0.1, 0.3, 0.5, 0.7, 0.9$. Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "- Apply 5-fold cross-validation to choose the optimal combination of the dropout rate and number of trees. How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?\n",
    "[hint: Training with large number of trees can take long time. You may need to restrict the max number of trees.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_function_forest(B, p, md=None):\n",
    "    test_counts = np.zeros((y_test.shape[0], B))\n",
    "    train_counts = np.zeros((y_train.shape[0], B))\n",
    "    for i in range(B):\n",
    "        train_set = X_train.copy()\n",
    "        for column in train_set.columns:\n",
    "            if np.random.uniform() < p:\n",
    "                train_set[column] = 0\n",
    "        test_counts[:,i] = (DecisionTreeClassifier(max_depth=md).fit(train_set, y_train)).predict(X_test)\n",
    "        train_counts[:,i] = (DecisionTreeClassifier(max_depth=md).fit(train_set, y_train)).predict(X_train)\n",
    "    y_hat = (np.mean(test_counts,axis=1)>.5).astype(float)\n",
    "    y_hat_train = (np.mean(train_counts,axis=1)>.5).astype(float)\n",
    "    return (metrics.accuracy_score(y_test, y_hat), metrics.accuracy_score(y_train, y_hat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.67479999999999996, 1.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_function_forest(100, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kimia\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (15pt): Random Forests\n",
    "\n",
    "We now move to a more sophisticated ensemble technique, namely random forest:\n",
    "1. How does a random forest approach differ from the dropout procedure described in Question 2? \n",
    " \n",
    "- Fit random forest models to the training set for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracies of the models. You may set the number of predictors for each tree in the random forest model to $\\sqrt{p}$, where $p$ is the total number of predictors. \n",
    "\n",
    "- Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier. \n",
    "  \n",
    "- Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the  number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15pt): Boosting\n",
    "\n",
    "We next compare the random forest model with the approach of boosting:\n",
    "\n",
    "\n",
    "1. Apply the AdaBoost algorithm to fit an ensemble of decision trees. Set the learning rate to 0.05, and try out different tree depths for the base learners: 1, 2, 10, and unrestricted depth.  Make a plot of the training accuracy of the ensemble classifier as a function of tree depths. Make a similar plot of the test accuracies as a function of number of trees (say $2, 4, 8, 16, \\ldots, 256$).\n",
    "- How does the number of trees influence the training and test performance? Compare and contrast between the trends you see in the training and test performance of AdaBoost and that of the random forest models in Question 3. Give an explanation for your observations.\n",
    "- How does the tree depth of the base learner impact the training and test performance? Recall that with random forests, we allow the depth of the individual trees to be unrestricted. Would you recommend the same strategy for boosting? Explain your answer.\n",
    "- Apply 5-fold cross-validation to choose the optimal number of trees $B$ for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (3pt): Meta-classifier\n",
    "\n",
    "We have so far explored techniques that grow a collection of trees either by creating multiple copies of the original training set, or through a sequential procedure, and then combines these trees into a single classifier. Consider an alternate scenario where you are provided with a pre-trained collection of trees, say from different participants of a data science competition for Higgs boson discovery. What would be a good strategy to combine these pre-fitted trees into a single powerful classifier? Of course, a simple approach would be to take the majority vote from the individual trees. Can we do better than this simple combination strategy?\n",
    "\n",
    "A collection of 100 decision tree classifiers is provided in the file `models.npy` and can be loaded into an array by executing:\n",
    "\n",
    "`models = np.load('models.npy')`\n",
    "\n",
    "You can make predictions using the $i^\\text{th}$ model on an array of predictors `x` by executing:\n",
    "\n",
    "`model[i].predict(x)`  &nbsp;&nbsp;&nbsp;\n",
    "or &nbsp;&nbsp;&nbsp;\n",
    "`model[i].predict_proba(x)`\n",
    "\n",
    "and score the model on predictors `x` and labels `y` by using:\n",
    "\n",
    "`model[i].score(x, y)`.\n",
    "\n",
    "1. Implement a strategy to combine the provided decision tree classifiers, and compare the test perfomance of your approach with the majority vote classifier. Explain your strategy/algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
